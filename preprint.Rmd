---
title             : "Spotting Fake News and Doubting True News: A Meta-Analysis of News judgements"
shorttitle        : "A Meta-Analysis of News judgements"

header-includes:  
  - \usepackage{fancyhdr} # some work around to get rid of the shorttitle/running head
  - \pagestyle{fancy}
  - \fancyhead{}
  - \renewcommand{\headrulewidth}{0pt}
  - \rhead{}


author: 
  - name          : "Jan Pfänder"
    affiliation   : "1"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - "Conceptualization"
      - "Data collection"
      - "Statistical Analyses"
      - "Writing - Original Draft"
      - "Writing - Review & Editing"
  - name          : "Sacha Altay"
    affiliation   : "2"
    corresponding : yes    # Define only one corresponding author
    address       : " "
    email         : "sacha.altay@gmail.com"
    role:
      - "Conceptualization"
      - "Data collection"
      - "Writing - Original Draft"
      - "Writing - Review & Editing"
      - "Supervision"

affiliation:
  - id            : "1"
    institution   : "Institut Jean Nicod, Département d’études cognitives, ENS, EHESS, PSL University, CNRS"
  - id            : "2"
    institution   : "University of Zurich"

authornote: |
  JP received funding from the SCALUP ANR grant ANR-21-CE28-0016-01

  SA received funding XX

abstract: |
  XX
  
keywords          : "keywords"
wordcount         : "X"

bibliography      : bibliography.bib
csl               : nature.csl

floatsintext      : yes
linenumbers       : no 
draft             : no
mask              : no

figurelist        : no
tablelist         : no
footnotelist      : no

classoption       : "man"
output            : papaja::apa6_doc # "_doc" for word; however, note that some of the features of kableExtra are not available for word and will yield errors. For example to knit to word, you'll have to comment out all "add_header_above()" functions for model output tables

appendix:
  - "appendix_a.Rmd"
  - "appendix_b.Rmd"
  - "appendix_c.Rmd"
  - "appendix_d.Rmd"
  - "appendix_e.Rmd"
  - "appendix_f.Rmd"
  - "appendix_g.Rmd"
---

```{r setup, include=FALSE}
# Figure out output format
is_docx <- knitr::pandoc_to("docx") | knitr::pandoc_to("odt")
is_latex <- knitr::pandoc_to("latex")
is_html <- knitr::pandoc_to("html")

# Word-specific things
table_format <- ifelse(is_docx, "huxtable", "kableExtra")  # Huxtable tables
conditional_dpi <- ifelse(is_docx, 300, 300)  # Higher DPI
conditional_align <- ifelse(is_docx, "default", "center")  # Word doesn't support align

# Knitr options
knitr::opts_chunk$set(
  echo = FALSE, warning = FALSE, message = FALSE,
  # tidy.opts = list(width.cutoff = 120),  # Code width
  # fig.retina = 3, dpi = conditional_dpi,
  # fig.width = 7, fig.asp = 0.618,
  # fig.align = conditional_align, out.width = "100%",
  fig.path = "output/figures/",
  cache.path = "output/_cache/",
  fig.process = function(x) {  # Remove "-1" from figure names
    x2 = sub('-\\d+([.][a-z]+)$', '\\1', x)
    if (file.rename(x, x2)) x2 else x
  },
  options(scipen = 99999999)  # Prevent scientific notation
)

# R options
options(
  width = 90,  # Output width
  dplyr.summarise.inform = FALSE,  # Turn off dplyr's summarize() auto messages
  knitr.kable.NA = "",  # Make NAs blank in kables
  kableExtra.latex.load_packages = FALSE,  # Don't add LaTeX preamble stuff
  modelsummary_factory_default = table_format,  # Set modelsummary backend
  modelsummary_format_numeric_latex = "plain"  # Don't use siunitx
)
```

```{r packages, include=FALSE}
# load required packages
library("papaja")      # For APA style manuscript   
library("lme4")        # model specification / estimation
library("lmerTest")    # provides p-values in the output
library("tidyverse")   # data wrangling and visualisation
library("afex")        # anova and deriving p-values from lmer
library("broom")       # extracting data from model fits 
library("broom.mixed") # extracting data from mixed models
library("metafor")     # doing mata analysis
library("metaviz")     # vizualization of meta analysis
library("stringr")     # for dmetar p-curve function to work
library("poibin")      # for dmetar p-curve function to work
library("patchwork")   # put several plots together
library("ggridges")    # for plots
library("gghalves")    # for plots
library("ggbeeswarm")  # Special distribution-shaped point jittering
library("knitr")       # for tables
library("kableExtra")  # also for tables
library("sf")          # for maps
library("ggpubr")      # for combining plots with ggarrange() 
library("viridis")     # for generating color palette for map 


# load required functions from dmetar package
source("functions/variance_composition_function.R") # calculates variance composition for metafor output
source("functions/p_curve_dmetar_function.R")       # generates p_curve and according estimates

# load own functions
source("functions/own_functions.R")

# load plot theme
source("functions/plot_theme.R") 
```

```{r read-data}
# load data
meta_wide <- read_csv("./data/cleaned.csv") %>% 
  # control conditions only
  filter(condition == "control")
```

```{r compute-correlation, echo=FALSE, message=FALSE}
# get average correlation for samples from the raw data subset
average_correlation <- read_csv("data/correlations_by_sample.csv") %>% 
  summarise(mean_r = mean(r)) %>% pull(mean_r)

# add a column to the data frame that identifies the correlation
meta_wide <- meta_wide %>% 
  mutate(cor = average_correlation)
```

```{r calculate-effects}
# calculate main effect sizes (SMCC) 
accuracy_effect <- calculate_effect_sizes(effect = "accuracy")
error_effect <- calculate_effect_sizes(effect = "error")

# calculate alternative effect sizes
# SMCR
SMCR_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "SMCR")
SMCR_error_effect <- calculate_effect_sizes(effect = "error", measure = "SMCR")
# SMD
SMD_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "SMD")
SMD_error_effect <- calculate_effect_sizes(effect = "error", measure = "SMD")
```

```{r models}
# Main models

# SMCC (main, pre-registered analysis, using change score standardization)
robust_model_accuracy <- calculate_models(data=accuracy_effect, robust = TRUE)
robust_model_error <- calculate_models(data=error_effect, robust = TRUE)

# store results in list
list_model_accuracy <- tidy(robust_model_accuracy, conf.int = TRUE) %>% 
  round_numbers() %>% 
  mutate(ci = glue::glue("[{conf.low}, {conf.high}]")) %>%
  split(.$term)
list_model_error <- tidy(robust_model_error, conf.int = TRUE) %>% 
  round_numbers() %>% 
  mutate(ci = glue::glue("[{conf.low}, {conf.high}]")) %>%
  split(.$term)

# calculate prediction intervals
predict_model_accuracy <- predict(robust_model_accuracy)
predict_model_error <- predict(robust_model_error)

# Models using alternative effect sizes (for comparison)

# SMCR (standardized mean change using raw score standardization)
SMCR_model_accuracy <- calculate_models(data=SMCR_accuracy_effect, robust = TRUE, 
                                        measure = "SMCR")
SMCR_model_error <- calculate_models(data=SMCR_error_effect, robust = TRUE, 
                                     measure = "SMCR")

# SMD (standardized mean change assuming independence)
SMD_model_accuracy <- calculate_models(data=SMD_accuracy_effect, robust = TRUE,
                                          measure = "SMD")
SMD_model_error <- calculate_models(data=SMD_error_effect, robust = TRUE, 
                                    measure = "SMD")
```

```{r variance-composition}
# variance composition

# Main models
variance_composition <- var.comp(robust_model_accuracy)
variance_composition_accuracy <- variance_composition$results %>% 
  mutate_if(is.numeric, round, 2) %>% 
  mutate(outcome = "accuracy")

variance_composition <- var.comp(robust_model_error)
variance_composition_error <- variance_composition$results %>% 
    mutate_if(is.numeric, round, 2) %>% 
  mutate(outcome = "error")

I2 <- rbind(variance_composition_accuracy, variance_composition_error) %>% 
  rename(share_variance = `% of total variance`) %>% 
  mutate(share_variance = paste0(share_variance, "%")) %>% 
  split(.$outcome)

# removing studies distinguishing between concordance
error_without_concordance <- calculate_models(data=error_effect %>% 
                                                filter(is.na(political_concordance)), 
                                              robust = TRUE)

variance_composition <- var.comp(error_without_concordance)
variance_error_without_concordance <- variance_composition$results %>% 
    mutate_if(is.numeric, round, 2) %>% 
  rename(share_variance = `% of total variance`) %>% 
  mutate(share_variance = paste0(share_variance, "%"), 
         outcome = "error")
```

```{r calculate-moderators}
# Moderator Models

# calculate models
moderator_models_accuracy <- calculate_moderator_models(data = accuracy_effect)
moderator_models_error <- calculate_moderator_models(data = error_effect)

# make more convenient lists of model output for reporting
list_moderators_accuracy <- calculate_moderator_models(data = accuracy_effect, list_report = TRUE)
list_moderators_error <- calculate_moderator_models(data = error_effect, list_report = TRUE)
```

```{r descriptives}
# make objects for descriptive reporting and put them in a list
descriptives <- list(
  
  papers = meta_wide %>% summarize(n_distinct(paperID)) %>% pull(), 
  
  samples = meta_wide %>% summarize(n_distinct(unique_sample_id)) %>% pull(),
  
  participants = meta_wide %>% 
    group_by(unique_sample_id) %>% 
    summarise(participants_per_sample = max(n_subj)) %>% 
    summarize(sum(participants_per_sample)) %>% 
    pull() %>% 
    # uneven numbers may occur where we didn't have information on how many 
    # participants in treatment vs. control group and assumed an even split 
    # (i.e. dividing overal n by number of condition)
    round(digits = 0), 
  
  participants_per_country = meta_wide %>% 
    group_by(unique_sample_id, country_grouped) %>% 
    summarise(participants_per_sample = max(n_subj)) %>% 
    group_by(country_grouped) %>% 
    summarize(n_subj = sum(participants_per_sample)) %>%
    # uneven numbers may occur where we didn't have information on how many 
    # participants in treatment vs. control group and assumed an even split 
    # (i.e. dividing overal n by number of condition)
    mutate_if(is.numeric, round, digits = 0) %>% 
    mutate(share = round(n_subj / sum(n_subj), digits=2), 
           share = paste0(share*100, "%")) %>% 
    split(.$country_grouped),
  
  observations = meta_wide %>% summarize(sum(as.integer(n_observations))) %>% pull(),
  
  effects = nrow(meta_wide),
  
  news = meta_wide %>% 
    # choosing most conservative way of estimating by limiting to original news 
    # headlines only (not recycled and not partly recycled)
    filter(recycled_news == "original") %>% 
    pivot_longer(c(n_news, n_news_pool), 
                 names_to = "pool", 
                 values_to = "n") %>% 
    group_by(unique_news_id) %>% 
    summarise(n = max(n, na.rm = TRUE)) %>% 
    summarise(sum(n)) %>% 
    pull(), 
  
  news_per_participant = meta_wide %>% 
    group_by(unique_news_id, unique_sample_id) %>% 
    summarise(n_per_sample_participant = max(n_news, na.rm = TRUE)) %>% 
    group_by(unique_sample_id) %>% 
    summarise(n = sum(n_per_sample_participant)) %>% 
    summarize(mean = mean(n),
              median = median(n),
              max = max(n),
              min = min(n)) %>% 
    round(digits = 2),
  
  news_pool_n = meta_wide %>% 
    mutate(pool = ifelse(!is.na(n_news_pool), "yes", "no")) %>% 
    group_by(pool) %>% 
    summarize(n_samples = n_distinct(unique_sample_id), 
              n_effects = n_distinct(observation_id)
    ) %>% 
    split(.$pool),
  
  news_pool_distribution = meta_wide %>% 
    group_by(unique_sample_id) %>% 
    summarize(n_pool = max(n_news_pool)) %>% 
    drop_na(n_pool) %>% 
    summarize(max = max(n_pool), 
              min = min(n_pool),
              mean = mean(n_pool)) %>% 
    round(digits = 2),
  
  countries = meta_wide %>% reframe(unique(country)) %>% nrow(), 
  
  continents = meta_wide %>% reframe(unique(continent)) %>% nrow(),
  
  discernment = accuracy_effect %>% 
    mutate(direction = ifelse(yi <= 0, "below/equal0", "above0")) %>% 
    group_by(direction) %>% 
    summarize(n_effect = n_distinct(observation_id)) %>% split(.$direction),
  
  error = error_effect %>% 
    mutate(direction = ifelse(yi <= 0, "below/equal0", "above0")) %>% 
    group_by(direction) %>% 
    summarize(n_effect = n_distinct(observation_id)) %>% split(.$direction), 
  
  design = meta_wide %>% group_by(design) %>% 
    summarise(n = n_distinct(observation_id)) %>% split(.$design),
  
  concordance = meta_wide %>% filter(news_family == "politically_concordant") %>% 
    summarise(n_sample = n_distinct(unique_sample_id), 
              n_paper = n_distinct(paperID), 
              n_effect = n_distinct(observation_id)
    ) %>% pivot_longer(everything()) %>% split(.$name), 
  
  online = meta_wide %>% 
    group_by(online) %>% 
    summarize(n = n_distinct(observation_id)) %>% split(.$online), 
  
  peer_review = meta_wide %>% group_by(peer_reviewed) %>% 
    summarize(n_papers = n_distinct(paperID)) %>% split(.$peer_reviewed), 
  
  data_descriptive_plot = plot_descriptive(return = "data") %>% split(.$veracity),
  
  format = meta_wide %>% group_by(news_format_grouped) %>% summarise(across(c(paperID, unique_sample_id, observation_id), 
                                                                            ~ n_distinct(.x), .names = "n_{.col}"
  )
  ) %>% split(.$news_format_grouped),
  
  source = meta_wide %>% group_by(news_source) %>% 
    mutate(news_source = ifelse(news_source == TRUE, "source", "no_source")) %>% 
    summarise(across(c(paperID, unique_sample_id, observation_id), 
                     ~ n_distinct(.x), .names = "n_{.col}")
    ) %>% split(.$news_source),
  
  country = meta_wide %>% group_by(country_grouped) %>% 
    summarise(across(c(paperID, unique_sample_id, observation_id), 
                     ~ n_distinct(.x), .names = "n_{.col}"
    )
    ) %>% split(.$country_grouped), 
  
  n_subj_by_sample = meta_wide %>% group_by(unique_sample_id) %>% 
    summarise(n_subj = max(n_subj)) %>% 
    summarize(max = max(n_subj), 
              min = min(n_subj),
              mean = mean(n_subj), 
              median = median(n_subj)) %>% 
    mutate_if(is.numeric, round, digits = 2), 
  
  perfect_symmetry = meta_wide %>% 
    mutate(perfect_symetry = ifelse(perfect_symetry == TRUE, "perfect",
                                    "not_perfect"), 
           perfect_symetry = ifelse(is.na(perfect_symetry), "not_enough_info", perfect_symetry)
           ) %>% 
    group_by(perfect_symetry) %>% summarize(n = n_distinct(observation_id)) %>% 
    ungroup() %>% mutate(share = round(n / sum(n), digits=2),
                         share = paste0(share*100, "%")) %>% 
    split(.$perfect_symetry), 
  
  significant_discernment = accuracy_effect %>% 
    mutate(conf_low = yi - 1.96*sqrt(vi), 
           conf_high = yi + 1.96*sqrt(vi),
           significant = ifelse((conf_low < 0 & conf_high < 0) |
                                  (conf_low > 0 & conf_high > 0), 
                                "significant", 
                                "not_significant"
           ), 
           direction = ifelse(yi > 0, "positive", "negative")
    ) %>% 
    group_by(significant, direction) %>% 
    summarise(n = n_distinct(observation_id)) %>% 
    super_split(direction, significant),
  
  significant_bias = error_effect %>% 
    mutate(conf_low = yi - 1.96*sqrt(vi), 
           conf_high = yi + 1.96*sqrt(vi),
           significant = ifelse((conf_low < 0 & conf_high < 0) |
                                  (conf_low > 0 & conf_high > 0), 
                                "significant", 
                                "not_significant"
           ), 
           direction = ifelse(yi > 0, "positive", "negative")
    ) %>% 
    group_by(significant, direction) %>% 
    summarise(n = n_distinct(observation_id)) %>% 
    super_split(direction, significant)
  
)

```

```{r lit-review, include=FALSE, message=FALSE}
# Literature review

# load prisma data
prisma <- read_csv("literature_search/prisma.csv")

# turn data frame into list to be able to call easily using inline coding
prisma <- as.list(prisma %>% pivot_wider(names_from = category, values_from = n))
```

# Abstract

How good are people at judging the veracity of news? We conducted a systematic literature review and pre-registered meta-analysis of `r descriptives$effects` effect sizes from `r descriptives$papers` experimental papers evaluating accuracy ratings of true and false news ($N_{participants}$ = `r descriptives$participants` from `r descriptives$countries` countries across `r descriptives$continents` continents). We found that people rated true news as much more accurate than false news (d = `r list_model_accuracy$overall$estimate`, 95% CI = `r list_model_accuracy$overall$ci`) and are slightly better at rating false news as inaccurate than at rating true news as accurate (d = `r list_model_error$overall$estimate`, 95% CI = `r list_model_error$overall$ci`). The political concordance of the news had no effect on discernment, but participants were more skeptical of politically discordant news. These findings lend support to crowdsourced fact-checking initiatives, and suggest that, to improve discernment, there is more room to increase the acceptance of true news than to reduce the acceptance of false news.

# Introduction

Many have expressed concerns that we live in a "post-truth" era, and that people cannot tell the truth from falsehoods anymore. In parallel, populist leaders around the world have tried to erode trust in the news by delegitimizing journalists and media outlets. Since the 2016 US presidential election, over 4000 scientific articles have been published on the topic of false news [REF]. Across the world, numerous experiments evaluating the effect of interventions against misinformation or susceptibility to misinformation have relied on a similar design feature: having participants rate the accuracy of true and false headlines--typically in a Facebook-like format, with an image, title, lede, and source, or as an isolated title/claim. These studies allow us to shed some light on the most common fears voiced about false news, namely that people may fall for false news, distrust true news, or be unable to discern between the true and false news. In particular, we investigated whether people rate true news as more accurate than false news (discernement) and whether they were better at rating false news as inacurrate than at rating true news as accurate (response bias). We also investigated various moderators of discernment and response bias such as political congruence, the topic of the news, or the presence of a source. Establishing whether people can spot false news is important to design interventions against misinformation: if people lack the skills to spot false news, interventions should be targeted at improving skills to detect false news, whereas if people have the ability to spot false news but nonetheless engage with it, then the problem lies elsewhere and may be one of motivation or attention that educational interventions may struggle to address. Past work has reliably shown that people do not fare better than chance at detecting lies because most verbal and non-verbal cues people use to detect lies are unreliable [@brennenLieDetectionWhat2023]. Why would this be any different for detecting false news? People make snap judgments to evaluate the quality of the news they come across [@montalverneTrustGapHow2022], and rely on seemingly imperfect proxies such as the source of information, police and fonts, the presence of hyperlinks, the quality of visuals, ads, or the tone of the text [@metzgerMakingSenseCredibility2007; @rossarguedasSnapJudgementsHow2022]. In experimental settings participants report relying on intuitions and tacit knowledge to judge the accuracy of news headlines [@altayExposureHigherRates2023]. Yet, a scoping review of the literature on belief in fake news (including a total of 26 articles) has shown that in experiments participants "can detect deceitful messages reasonably well" [@bryanovDeterminantsIndividualsBelief2021, p. 19]. Similarly, a survey on 150 misinformation experts has shown that 53% of experts agree that "people can tell the truth from falsehoods" -- while only 25% of experts disagreed with the statement [@altayExposureHigherRates2023]. Unlike the unreliable proxies people rely on to detect lies in interpersonal contexts, there are reasons to believe that some of the cues people use to detect false news may, on average, be reliable. For instance, the news outlets people trust the less do publish more false news, as people's trust ratings of news outlets correlated strongly with fact-checkers ratings in the US and Europe [@pennycookLazyNotBiased2019; @schulzAreNewsOutlets2020]. Thus, source cues will work on average. Moreover, false news has some distinctive properties, such as being more politically slanted [@mouraoFakeNewsDiscursive2019], being more novel, surprising, or disgusting, being more sensationalist, funnier, less boring, and less negative [@vosoughiSpreadTrueFalse2018a; @chenWhatMakesNews2023], or being more interesting-if-true [@altayIfThisAccount2022]. These features are designed to increase engagement but do so at the expense of accuracy, and in many cases people may pick up on it. This led us to pre-register the hypothesis that people would rate true news as more accurate than false news. Yet, legitimate concerns have been raised about the lack of data outside of the US, especially in some Global South countries where the misinformation problem is arguably worst [REF]. Our meta-analysis covers `r descriptives$countries` countries across `r descriptives$continents` continents and directly addresses concerns about the over-representation of US-data.

### H1: People rate true news as more accurate than fake news.


While many fear that people are exposed to too much misinformation, too easily fall for it, and are overly influenced by it, a growing body of researchers is worried that people are exposed to too little reliable information, commonly reject it, and are excessively resistant to it [@acerbiResearchNoteFighting2022; @mercierNotBornYesterday2020]. Establishing whether true news skepticism (excessively rejecting true news) is of similar magnitude to false news gullibility (excessively accepting false news) is important for future studies on misinformation: if people are excessively gullible interventions should primarily aim at fostering skepticism, whereas if people are excessively skeptical interventions should focus on increasing trust in reliable information. For these reasons, in addition to investigating discernment (H1), we also looked at response bias by comparing the magnitude of true news skepticism to false news gullibility.
Research in psychology has shown that people exhibit a "truth bias" [@brashierJudgingTruth2020; @streetSourceTruthBias2015], such that they tend to accept incoming statements. Similarly, work on interpersonal communication has shown that, by default, people tend to accept communicated information [@levineTruthDefaultTheoryTDT2014] . However, there are reasons to be skeptical that the truth-default-theory holds for news judgments. It has been hypothesized that people display a truth bias in interpersonal context because information in these contexts is often true and people thus expect communicated information to be true [@brashierJudgingTruth2020]. When it comes to the news judgments, it is not clear that people by default expect news stories to be true. Trust in the news and journalists is low worldwide [@newmanReutersInstituteDigital2022], and a significant part of the population hold cynic views of the news [@mihailidisCostDisbeliefFracturing2021] . Similarly, populists leaders across the world have attacked the credibility of the news media and instrumentalized the concept of fake news to discredit quality journalism [@egelhoferFakeNewsTwodimensional2019; @vanduynPrimingFakeNews2019]. Disinformation strategies such as "flooding the zone" with false information [@paulRussianFirehoseFalsehood2016; @ulusoyFloodingZoneHow2021] have been shown to increase skepticism in news judgments [@altayExposureHigherRates2023]. Moreover, in many studies included in our meta-analysis, the news stories were presented in a social media format (most often Facebook), which could fuel skepticism in news judgments. Indeed, people trust information on social media less than information on news websites [@fletcherPeopleDontTrust2017] , and trust news on social media less than news on news websites [@montalverneTrustGapHow2022]. In line with these observations, some empirical evidence suggests that for news judgments, people display the opposite of a truth bias [@luoCredibilityPerceptionsDetection2022], namely a conservative response bias, whereby people tend to rate all news as more false than they are [@altayExposureHigherRates2023; @bataillerSignalDetectionApproach2022; @modirrousta-galianGamifiedInoculationInterventions2023]. We thus predicted that participants will err on the side of skepticism more than on the side of gullibility. Precisely, we predicted that people will be better at rating false news as false than rating true news as true.

### H2: People are better at rating false news as false than true news as true.


Finally, we investigated potential moderators of H1 and H2, such as the country where the experiment was conducted, the format of the news headlines, the topic, whether the source of the news was displayed, and political concordance of the news. Past work has suggested that displaying the source of the news has a small effect at best on accuracy ratings [@diasEmphasizingPublishersDoes2020], whereas little work has investigated differences in news judgements across countries, topics, and format. The effect of political concordance on news judgements is debated.  Participants may be motivated to believe politically congruent (true and false) news, motivated to disbelieve politically incongruent news, or not be politically motivated at all but still display such biases  [@tappinBayesianBiasedAnalytic2020]. We formulated research questions instead of hypotheses for our moderator analyses because of a lack of strong theoretical arguments for predicting effects.

To test these hypotheses, we conducted a systematic literature review and pre-registered meta-analysis based on `r prisma$studies_included` publications, providing data on `r descriptives$samples` samples (`r descriptives$participants` participants) and `r descriptives$effects` effects (i.e. **k**, the meta analytic observations)[^1] For a publication to be included in our meta-analysis, we set seven eligibility criteria: (1) We considered relevant all document types with original data (not only published ones, but also reports, pre-prints and working papers). When different publications were using the same data, a scenario we encountered various times, we included only one publication (which we picked arbitrarily). (2) We only considered papers that measured the perceived accuracy, and (3) did so for both fake and true news. Our interpretation of 'accuracy' was liberal, including not only studies that were asking participants about how "accurate", but also those asking how "credible", "trustworthy", "reliable" or "manipulative" news items were. (4) We required that studies need to rely on real-world news items (i.e. present participants with fake and true news that actually circled). Accordingly, we excluded studies in which researchers had made up their own fake news, or manipulated properties of true news. (5) We could only include those publications that we could access (almost never an issue) and (6) those that provided us with the relevant summary statistics (means and standard deviations for both fake and true news), or publicly available data that allowed us to calculate those. In cases where we were not able to retrieve the relevant summary statistics either way, but the study was otherwise relevant, we contacted the authors. (7) Finally, to ensure comparability, we only included studies that provided a neutral control condition, excluding for example experiments with 2x2 designs. After starting the literature search, we added further search criteria in order to diminish the vast number of results (see [methods]). Rejection decisions for all retrieved papers are documented on the OSF.

[^1]: Sometimes a sample provides several effect sizes, for example, when separate accuracy ratings are available by news topic, or when follow-up studies were conducted on the same participants. We account for the resulting hierarchical structure of the data in our statistical models.

We found that on average people are good at discerning true from false news, and rate true news as much more accurate than false news. However, they are slightly better at rating false news as inaccurate than at rating true news as accurate, and err on the side of skepticism by rating all news as more inaccurate.

# Results

## Descriptives

Our meta-analysis includes publications from `r descriptives$countries` countries and `r descriptives$continents` continents. However, the number of participants varies a lot, led by the United States with `r descriptives$participants_per_country$US$n_subj` participants (`r descriptives$participants_per_country$US$share` of all participants) (see Fig. \@ref(fig:map)).

The average sample size was `r descriptives$n_subj_by_sample$mean` (min = `r descriptives$n_subj_by_sample$min`, max = `r descriptives$n_subj_by_sample$max`, median = `r descriptives$n_subj_by_sample$median`)[^2].

[^2]: Uneven numbers for sample sizes are due to cases when we didn't have information on how many participants there were in each experimental condition. In such cases, we took the overall reported sample size and assumed an even split across conditions (i.e. dividing the overall N by the number of conditions)

Overall, participants rated the accuracy of `r descriptives$news` unique news items. On average, a participant rated `r descriptives$news_per_participant$mean` news items per study (min = `r descriptives$news_per_participant$min`, max = `r descriptives$news_per_participant$max`, median = `r descriptives$news_per_participant$median`). For `r descriptives$news_pool_n$yes$n_samples` samples, news items were sampled from a pool of news (pool size ranging from `r descriptives$news_pool_distribution$min` to `r descriptives$news_pool_distribution$max`, average pool size = `r descriptives$news_pool_distribution$mean` items). The vast majority of studies (`r descriptives$design$within$n` out of `r descriptives$effects` effects) used a within participant design for manipulating news veracity, with each participant rating both true and fake news items. Almost all effect sizes are from online studies (`r descriptives$online$yes$n` out of `r descriptives$online$yes$n + descriptives$online$no$n`).

(ref:map) A map of the number of effect sizes per country.

```{r map, fig.cap="(ref:map)"}
plot_map(fill = "effects")
```

### Analytic procedures

All analyses were pre-registered and choice of models informed by simulations. To test H1, we calculated a discernment score by subtracting the mean accuracy ratings of false news from the mean accuracy ratings of true news, such that higher scores indicate better discernment. To test H2, we first calculated a judgement error for true and fake news, respectively. Generally, we define error as the distance of observed judgement performance of a sample to the best possible performance. For false news, error is defined as the distance of the accuracy value to the bottom of the scale (e.g. average accuracy of fake news in one sample is `2.2` and the accuracy scale goes from `1`, not accurate at all, to `4`, completely accurate, then the error is `2.2 - 1 = 1.2`). For true news, error is defined as the distance of the accuracy value to the top of the scale (e.g. average accuracy of fake news in one sample is `2.5` and the accuracy scale goes from `1`, not accurate at all, to `4`, completely accurate, then the error is `4 - 2.5 = 1.5`). We then calculate the response bias as the difference between the two errors, subtracting the true news error score from the false news error score (e.g. response bias 1.5 - 1.2 = 0.3).

For a descriptive (irrespective of sample sizes) overview, we mapped the raw mean accuracy ratings for true and fake news that we used to calculate effect sizes onto a scale from 0 to 1 and plotted their distributions (Fig. \@ref(fig:descriptive)). On average, true news were rated as more accurate than false news, as shown by the positive discernment score (`r descriptives$data_descriptive_plot$true$discernment`). We also see that false news discrimination is better than true news discrimination, i.e., the distance between true news ratings and the top of the scale (`r descriptives$data_descriptive_plot$true$error_true`) is greater than the distance between false news ratings and bottom of the scale (`r descriptives$data_descriptive_plot$true$error_fake`), yielding a positive response bias (`r descriptives$data_descriptive_plot$true$error_true` - `r descriptives$data_descriptive_plot$true$error_fake` = `r descriptives$data_descriptive_plot$true$error_true - descriptives$data_descriptive_plot$true$error_fake`).

(ref:descriptive) Distribution of effect sizes for true and false news, scaled to range from 0 to 1. The figure illustrates discernment (the distance between the mean for true news and the mean for fake news) and the errors (distance to right end for true news, to left end for fake news) from which the response bias is computed. A larger error for true news compared to fake news yields a positive response bias. In this descriptive figure, unlike in the meta-analysis, effect sizes are not weighed by sample size.

```{r descriptive, fig.cap="(ref:descriptive)"}
plot_descriptive()
```

Discrimination and response bias can only (meaningfully) be computed on scales using symmetrical labels (e.g., "True" vs "False" or "Definitely fake" [1] to "Definitely real" [7]), while `r descriptives$perfect_symmetry$perfect$share` of effects included in the meta-analysis used scales with perfectly symmetrical labels, while `r descriptives$perfect_symmetry$not_perfect$share` used imperfectly symmetrical scale labels (e.g., [1] not at all accurate, [2] not very accurate, [3] somewhat accurate, [4] very accurate)[^3]. We do not find a difference regarding response bias between effects from studies using perfectly and imperfectly symmetrical scales, but we do find a difference regarding discernment (see appendix \@ref(moderators)). While studies with perfectly symmetric scales tend to yield lower discernment scores, our results hold across both types of scale. 

[^3]: We could only compute this variable for scales that explicitly labeled each scale point, resulting in missing values for `r descriptives$perfect_symmetry$not_enough_info$share` of effects.

We calculated standardized mean changes using change score standardization (SMCC) to compare effects [@gibbons_estimation_1993]. This measure expresses effects in units of (pooled) standard deviations allowing for comparison across different scales while accounting for the statistical dependence between true and fake news ratings arising from the within participant design used by most studies (`r descriptives$design$within$n` out of `r descriptives$effects` effects). An SMCC of one for discernment, for example, would mean that people rate true news as more accurate by one pooled standard deviation, with that standard deviation taking into account the correlation between true and fake news ratings (see [methods]).

For the remaining `r descriptives$design$between$n` effects from studies that used a between participant design, we calculated Hedge's g, a common measure of standardized mean difference that assumes independence between groups [@hedges_distribution_1981]. In appendix \@ref(effect-sizes), we show that the results hold across alternative effect measures, the SMCC yielding the most conservative estimates. We also provide effect estimates in units of the original scales separately for each scale.

We used multilevel meta models with clustered standard errors at the sample level to account for cases in which the same sample contributed various effect sizes (i.e. the meta-analytic units of observation).

### Main results

### Discernment (H1)

(ref:discernment) Forest plot for discernment. Effects are weighed by their sample size. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the sample level.

```{r discernment, echo=FALSE, fig.cap="(ref:discernment)"}
### create plot for accuracy
forest.rma(robust_model_accuracy,
       xlim = c(-0.5, 3),        ### adjust horizontal plot region limits
       at = c(-0.5, 0, 1, 2, 3), 
       order="obs",             ### order by size of yi
       slab=NA, annotate=FALSE, ### remove study labels and annotations
       efac=c(0, 1),            ### remove vertical bars at end of CIs
       pch=19,                  ### changing point symbol to filled circle
       col="gray40",            ### change color of points/CIs
       psize=1,                 ### increase point size
       cex.lab=0.8, cex.axis=0.8,   ### increase size of x-axis title/labels
       lty=c("solid", "dotted", "blank"),  ### remove horizontal line at top of plot
       mlab = "BLASt", 
       ylim = c(-22, 200), 
       addfit = FALSE, 
       xlab = "Standardized Mean Change Score (SMCC)") 
addpoly(robust_model_accuracy, mlab="Discernment", cex = 0.8, row = -20) 
abline(h=0)
# Add the text to the right-hand side of the polygon
text(x = list_model_accuracy$overall$estimate + 0.1, 
     y = -20,
     paste0(list_model_accuracy$overall$estimate," ",
            list_model_accuracy$overall$ci),
     pos = 4, cex = 0.7)

# # check limits
# accuracy_effect %>% summarize(min = min(yi),
#                            max = max(yi))
```

Supporting H1, participants rated true news as more accurate than false news on average. Pooled across all studies, the average discernment estimate is large (d = `r list_model_accuracy$overall$estimate` `r list_model_accuracy$overall$ci`). As shown in Fig. \@ref(fig:discernment), `r descriptives$discernment$above0$n_effect` of `r descriptives$effects` estimates are positive. Of the positive estimates, `r descriptives$significant_discernment$positive$not_significant$n` have a confidence interval that includes 0, as do `r descriptives$significant_discernment$negative$not_significant$n` of the negative estimates.
Most of the variance in the effect sizes observed above is explained by between-sample heterogeneity ($I2_{between}$ = `r I2$accuracy$share_variance[3]`). Within-sample heterogeneity is comparatively small ($I2_{within}$ = `r I2$accuracy$share_variance[2]`), indicating that when the same participants were observed on several occasions (i.e. the same sample contributed several effect sizes), on average, discernment performance was similar across those observations. The share of the variance attributed to sampling error is very small (`r I2$accuracy$share_variance[1]`), which is indicative of the large sample sizes and thus precise estimates.

### Response bias (H2)

(ref:bias) Forest plot for response bias. Effects are weighed by their sample size. Horizontal bars represent 95% confidence intervals. The average estimate is the result of a multilevel meta model with clustered standard errors at the sample level.

```{r bias, echo=FALSE, fig.cap="(ref:discernment)"}
### create plot for error
bias_plot <- forest.rma(robust_model_error,
       xlim = c(-2, 2),        ### adjust horizontal plot region limits
       at = c(-2, -1, 0, 1, 2), 
       order="obs",             ### order by size of yi
       slab=NA, annotate=FALSE, ### remove study labels and annotations
       efac=c(0, 1),            ### remove vertical bars at end of CIs
       pch=19,                  ### changing point symbol to filled circle
       col="gray40",            ### change color of points/CIs
       psize=1,                 ### increase point size
       cex.lab=0.8, cex.axis=0.8,   ### increase size of x-axis title/labels
       lty=c("solid", "dotted", "blank"),  ### remove horizontal line at top of plot
       mlab = "BLASt", 
       ylim = c(-22, 200), 
       addfit = FALSE, 
       xlab = "Standardized Mean Change Score (SMCC)")  
addpoly(robust_model_error, mlab="Response bias", cex = 0.8, row = -20)  
abline(h=0) 
# Add the text to the right-hand side of the polygon
text(x = list_model_error$overall$estimate + 0.1, 
     y = -20,
     paste0(list_model_error$overall$estimate," ",
            list_model_error$overall$ci),
     pos = 4, cex = 0.7)

# # check limits
# error_effect %>% summarize(min = min(yi),
#                            max = max(yi))
```

We found support for H2, with participants being better at rating false news as inaccurate than at rating true news as accurate (i.e. false news discrimination was on average higher than true news discrimination). However, the average response bias estimate is relatively small (d = `r list_model_error$overall$estimate` `r list_model_error$overall$ci`).

As shown in Fig \@ref(fig:bias), `r descriptives$error$above0$n_effect` of `r descriptives$effects` estimates are positive. Of the positive estimates, `r descriptives$significant_bias$positive$not_significant$n` have a confidence interval that includes 0, as do `r descriptives$significant_bias$negative$not_significant$n` of the negative estimates.

By contrast with discernment, the largest share of variance of the effects for response bias is explained by within-sample heterogeneity ($I2_{within}$ = `r I2$error$share_variance[2]`; $I2_{between}$ = `r I2$error$share_variance[3]`; sampling error = `r I2$error$share_variance[1]`). Whenever we observe within sample variation in our data, it is because several effects were available for the same sample. This is mostly the case for studies with multiple survey waves, or when effects were split by different news topics, suggesting that these factors account for some of that variation. In our meta-regression below, we compare across samples and broad categories, thereby likely glossing over much of that within variation. An exception is political concordance, a factor that has generally been manipulated within samples and in similar ways across studies.

### Moderators

Following the pre-registered analysis plan, we ran a separate regression for each moderator by adding the respective moderator variable as a fixed effect to the multilevel meta models. We report regression tables and visualizations in appendix \@ref(moderators).

#### Cross-cultural variability

For samples based in the United States (`r descriptives$country$US$n_observation_id`/`r descriptives$effects` effect sizes), discernment was on average higher than higher than for samples based in countries ($\Delta$ Discernment = `r list_moderators_accuracy$Country$country_groupedUS$estimate` `r list_moderators_accuracy$Country$country_groupedUS$ci`; Baseline discernment other countries pooled = `r list_moderators_accuracy$Country$intercept$estimate` `r list_moderators_accuracy$Country$intercept$ci`). However there was no statistically significant difference for response bias.

#### Scales

The studies in our meta analysis used a variety of response scales, including both binary (e.g. "Do you think the above headline is accurate? - Yes, No") and continuous ones (e.g. "To the best of your knowledge, how accurate is the claim in the above headline" 1 = Not at all accurate, 4 = Very accurate).

Regarding discernment, the only scale that differed from the most common four point scale (Baseline discernment 4-point-scale = `r list_moderators_accuracy$Scale$intercept$estimate` `r list_moderators_accuracy$Scale$intercept$ci`) was the six point scale, yielding lower discernment ($\Delta$ Discernment =`r list_moderators_accuracy$Scale$accuracy_scale_grouped6$estimate` `r list_moderators_accuracy$Scale$accuracy_scale_grouped6$ci`).

Regarding response bias, studies using a four point scale (Baseline response bias 4-point scale = `r list_moderators_error$Scale$intercept$estimate` `r list_moderators_error$Scale$intercept$ci`) reported larger effects for response bias compared to studies using a different scale (e.g. $\Delta$ response bias = `r list_moderators_error$Scale$accuracy_scale_groupedbinary$estimate` `r list_moderators_error$Scale$accuracy_scale_groupedbinary$ci` for binary scales; `r list_moderators_error$Scale$accuracy_scale_grouped7$estimate` `r list_moderators_error$Scale$accuracy_scale_grouped7$ci` for 7-point scales).

#### Format

We find increased response bias when studies used pictures ($\Delta$ response bias = `r list_moderators_error$Format$news_format_groupedheadline_picture$estimate` `r list_moderators_error$Format$news_format_groupedheadline_picture$ci`; `r descriptives$format$headline_picture$n_observation_id` effects) or pictures and a lede ($\Delta$ response bias = `r list_moderators_error$Format$news_format_groupedheadline_picture_lede$estimate` `r list_moderators_error$Format$news_format_groupedheadline_picture_lede$ci`; `r descriptives$format$headline_picture_lede$n_observation_id` effects) for stimuli to accompany news their headlines, compared to those using headlines only (Baseline response bias headlines only = `r list_moderators_error$Format$intercept$estimate` `r list_moderators_error$Format$intercept$ci`; `r descriptives$format$headline$n_observation_id` effects). We do not find differences related to format for discernment.

#### Topic

We do not find any differences associated with news topic, when distinguishing between political, covid-related and other news.

#### Sources

We do not find differences between instances where news items were presented alongside with a source (`r descriptives$source$source$n_observation_id` effects) and those that did not (`r descriptives$source$no_source$n_observation_id` effects; for `r descriptives$effects - (descriptives$source$source$n_observation_id + descriptives$source$no_source$n_observation_id)` this information was not explicitly provided).

#### Political Concordance

The moderators investigated so far varied across studies, not within studies, which impedes causal inference. For instance, very few studies manipulated the source experimentally. Instead, some studies provided news sources, while others did not. It is almost exclusively that between-studies variation that our statistical analyses pick up on.

Political concordance is an exception in this regard. It was manipulated within `r descriptives$concordance$n_sample$value` different samples, across `r descriptives$concordance$n_paper$value` different papers. In those experiments, typically, a pre-test establishes the political slant of news headlines (e.g. pro-republican vs. pro-democrat). In the main study, participants then rate the accuracy for news items of both political slants, and provide information about their own political stance. The ratings of items are then grouped into concordant or discordant (e.g. pro-republican news rated by republicans will be coded as concordant while pro-republican news rated by democrats will be coded as discordant).

Political concordance had no statistically significant effect on discernment. However, participants displayed a response bias only when rating politically discordant headlines (see Fig. \@ref(fig:concordance)). When rating concordant items, participants did not show response bias (Baseline response bias concordant items = `r list_moderators_error$Concordance$intercept$estimate` `r list_moderators_error$Concordance$intercept$ci`). Meanwhile, for discordant news items, participants displayed a positive response bias ($\Delta$ response bias = `r list_moderators_error$Concordance$political_concordancediscordant$estimate` `r list_moderators_error$Concordance$political_concordancediscordant$ci`). In other words, participants were not gullible when facing concordant news headlines (as would have suggested a negative response bias), but were skeptical when facing discordant ones.

(ref:concordance) Distribution of effect sizes for politically concordant and discordant items. The black dots represent the predicted average of the meta-regression, the black horizontal bars the 95% confidence intervals.

```{r concordance, fig.cap="(ref:concordance)"}
## plot effects
concordance_plot_accuracy <- plot_concordance(outcome = "accuracy")
concordance_plot_error <- plot_concordance(outcome = "error")

ggarrange(concordance_plot_accuracy, 
          concordance_plot_error + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank()))
```

# Discussion

This meta-analysis sheds light on some of the most common fears voiced about false news. In particular, we investigated whether people are able to discern true from false news, and whether they are better at discriminating true news or false news (response bias). Across `r descriptives$effects` effect sizes ($N_{participants}$ = `r descriptives$participants`) from `r descriptives$countries` countries across `r descriptives$continents` continents, we found that people rated true news as much more accurate than false news ($d_{discernment}$ = `r list_model_accuracy$overall$estimate` `r list_model_accuracy$overall$ci`) and are slightly better at rating false news as inaccurate than at rating true news as accurate ($d_{bias}$ = `r list_model_error$overall$estimate` `r list_model_error$overall$ci`).

The finding that people can discern true from false news when prompted to do so has important implications for interventions against misinformation. First, it suggests that most people do not lack the skills to spot false news. Instead, when they interact with false news, people may sometimes lack the motivation to use these skills or apply them selectively [@pennycook2021; @rathje2023]. Thus, instead of teaching people how to spot false news, it may be more fruitful to design interventions targeting motivations, either by manipulating features of the environment in which people encounter news [@capraroThinkThisNews; @globigChangingIncentiveStructure2023], or by intrinsically motivating people to use their skills and pay more attention to accuracy [@pennycook2021]. For instance, it has been shown that design features of current social media environments sometimes impede discernment [@epsteinSocialMediaContext2023]. Similarly, it has been suggested that interventions against misinformation should build on the tacit knowledge that people rely on to discriminate false news, instead of giving people explicit tips and guidelines that people may struggle to integrate in their tacit knowledge [@modirrousta-galianWordlessWisdomDominant2023].

Second, the fact that people can, on average, discern true from false news lends support to crowdsourced fact-checking initiatives. While fact-checkers cannot keep up with the pace of false news production, the crowd can, and it has been shown that even small groups of participants perform as well as fact-checkers [@allenScalingFactcheckingUsing2021; @martelCrowdsCanEffectively2022]. The cross-cultural scope of our findings suggests that these initiatives may be fruitful in most countries across the world. Indeed, we found that in every country included in the meta-analysis, participants on average rated true news as more accurate than false news see appendix \@ref(country-comparison).

The fact that people tend to disbelieve true news slightly more than they believe fake news  speaks to the nature of the misinformation problem and how to fight it. It is in line with the idea that the problem is less that people are gullible towards falsehoods, and more that they are skeptical towards reliable information [@altayMisinformationMisinformationConceptual; @mercierNotBornYesterday2020]. Even assuming that the rejection of true news and the acceptance of false news are of similar scale (and that both can be improved), given that true news are much more prevalent in people's news diet than false news [@allenEvaluatingFakeNews2020], true news skepticism may be more detrimental to the accuracy of people's beliefs than false news acceptance [@acerbiResearchNoteFighting2022]. This threat is real, as indicated by the low and declining trust in the news across the world [@newmanDigitalNewsReport2023], the attacks of populist leaders on the news media [@vanduynPrimingFakeNews2019], and growing news avoidance [@newmanDigitalNewsReport2023].

Our results stress that in the fight against misperceptions, there are two fronts: reducing the acceptance of false news and increasing the acceptance of true news [@acerbiResearchNoteFighting2022]. Whether policy makers should give one or the other greater priority will depend on additional factors that are beyond our scope to discuss here. At the very least, however, researchers who aim to provide solutions to the problem should investigate the effects of their interventions with regard to both types of news [@guayHowThinkWhether2022]. This is all the more important given that recent evidence suggest that many interventions against misinformation, such as media literacy tips [@hoesProminentMisinformationInterventions2023], fact-checking [@bachmannStudyingDownstreamEffects2023], or educational games aimed at inoculating people against misinformation [@modirrousta-galianGamifiedInoculationInterventions2023], may reduce misperceptions of false news at the cost of also reducing trust in true news.

We also investigated various moderators of discernment and response bias. We found that discernment was greater in studies conducted in the United States compared to the rest of the world. This could be due to the inclusion of many countries from the Global South, where belief in misinformation and conspiracy theories has been documented to be higher [@alperWhenConspiracyTheories2021]. In line with past work [@diasEmphasizingPublishersDoes2020], the presence of a source had no statistically significant effects. The topic of the news also had no statistically significant effects on discernment and response bias. Participants showed greater response bias towards skepticism in studies that presented headlines in a social media format (with an image and lede) or along with an image than in studies that used plain headlines. This suggests that the skepticism of true news documented in this meta-analysis may be partially due to the social media format. Past work has shown that people report trusting less news on social media [@montalverneTrustGapHow2022; @newmanReutersInstituteDigital2022], and experimental manipulations have shown that the Facebook news format reduces belief in news [@besaluCredibilityDigitalPolitical2021; @karlsenSocialMediaTrust2023] --although the causal effects documented in these experiments are much smaller than observational differences in reported trust levels between news on social media and on news outlets [@agadjanianPlatformPenaltyNews2023]. Lower trust of news on social media may be a good thing, given that on average news on social media may be less accurate than news on news websites, but is also worrying given that most of news consumption worldwide is shifting online and on social media in particular [@newmanDigitalNewsReport2023].

Finally, the political concordance of the news had no effect on discernment, but participants were excessively skeptical of politically discordant news. That is, participants were equally skilled at discerning true from false news for concordant and discordant items, but they rated news generally (true and false) as more false when politically discordant. This finding is in line with the idea that people are not excessively gullible of news they agree with, but are instead excessively skeptical of news they disagree with [@mercierNotBornYesterday2020; @troucheVigilantConservatismEvaluating2018]. It suggests that interventions aimed at reducing partisan motivated reasoning, or at improving political reasoning in general, should focus more on increasing openness to opposing viewpoints than on increasing skepticism towards concordant viewpoints.

Our meta-analysis has a number of conceptual limitations. First, in all studies participants had to rate the accuracy of the news stories, which may have increased discernment, given that prompting people to think about accuracy increases sharing discernment [@pennycook2021]. In the wild, when browsing on social media, people may be less discerning than in these experimental settings because they would pay less attention to accuracy [@epsteinSocialMediaContext2023]. However, given people's low exposure to misinformation online [@altayQuantifyingInfodemicPeople2022], most people may protect themselves from misinformation not by detecting misinformation on the spot, but by relying on the reputation of the sources and avoiding unreliable sources [@altayWhyFewPeople]. Second, accuracy ratings were averaged across participants and thus better reflect the wisdom of the crowd than the skills of individuals. Yet, past work [@allenScalingFactcheckingUsing2021] shows that most individuals appear able to discern true from false news better than chance. In line with this, we find that for those studies that  we have the raw data on, xx% of participants rated true news as higher than false news (TO D0!!!!). Third, the vast majority of studies in the meta-analysis relied on fact-checked false news. It is unclear whether the present findings generalize to non fact-checked false news and misinformation more broadly. For instance, it is likely that discerning true from misleading news is harder than discerning true from false news, and that people may discriminate true news better than misleading news [@aslettEcologicallyExternallyValid]. Fourth, response bias could be an artifact of biased news selection for experiments. For example, one might suspect researchers to pick easy-to-detect false news and/or hard-to-detect true news (e.g. to avoid ceiling effects), thus inflating participants skepticism of true news. However, we deem this unlikely (see appendix \@ref(selection-bias)).
Our meta-analysis further has methodological limitations which we address in a series of robustness checks in the appendix. We show that our results across a wide range of imputed correlation for our effect size estimator and that the resulting estimate is conservative with regard to alternative estimators (Appendix \@ref(effect-sizes)). We also show that we obtain similar results when running a participant-level analysis on a subset of studies for which we have raw data (Appendix \@ref(individual-level)) and when collapsing Likert scales into binary ones (Appendix \@ref(binary)).

In conclusion, we found that in experimental settings, people are able to discern true news from false news, but when they err they tend to do so on the side of skepticism more than on that of gullibility.  These findings lend support to crowdsourced fact-checking initiatives, and suggest that, to improve discernment, there may be more room to increase the acceptance of true news than to reduce the acceptance of false news.

(ref:prisma-flowchart) 2020 PRISMA flow diagram for new systematic reviews.

```{r prisma-flowchart, fig.cap="(ref:prisma-flowchart)"}
knitr::include_graphics("literature_search/PRISMA_2020_flow_diagram.pdf")
```

# Methods {#methods}

## Data

We undertook a systematic review and meta-analysis of the experimental literature on accuracy judgements of news, following the PRISMA guidelines [@page2021]. All records resulting from our literature searches can be found on the OSF. We documented rejection decisions for all retrieved papers. They, too, can be found on the OSF.

### Deviations from eligibility criteria

We strictly followed our eligibility criteria (as outlined above), with 4 exceptions. We rejected one paper based on a criterion that we had not previously set: scale asymmetry. @baptistaInfluencePoliticalIdeology2021 asked participants: "According to your knowledge, how do you rate the following headline?", providing a very asymmetrical set of answer options ("1---not credible; 2---somehow credible; 3---quite credible; 4---credible; 5---very credible"). The paper provides 6 effect sizes, all of which strongly favor our second hypothesis (one effect being as large as d = 2.54). To be conservative, we thus decided to exclude it from our analysis.
Further, we stretched our criterion for real-world news on three instances. @maertensMisinformationSusceptibilityTest2021 and @roozenbeekSusceptibilityMisinformationCOVID192020 used artificial intelligence trained on real-world news to generate fake news. @bryanovWhatDrivesPerceptions2023 created fake news relying on the expertise of journalists. We decided to include all three studies.

### Literature search

We first conducted as Scopus search (search string: *'"false news" OR "fake news" OR "false stor\*" AND "accuracy" OR "discernment" OR "credibilit\*" OR "belief" OR "susceptib\*"'*).

Given the high volume of papers (12425), we added restrictions to only include articles that were likely (i) experimental, (ii) and exposed participants to both true and false news (addition to search string: *'AND ( LIMIT-TO ( LANGUAGE , "English" ) ) AND ( LIMIT-TO ( DOCTYPE , "ar" ) OR LIMIT-TO ( DOCTYPE , "cp" ) ) AND ( EXCLUDE ( SUBJAREA , "PHYS" ) OR EXCLUDE ( SUBJAREA , "MATE" ) OR EXCLUDE ( SUBJAREA , "BIOC" ) OR EXCLUDE ( SUBJAREA , "ENER" ) OR EXCLUDE ( SUBJAREA , "IMMU" ) OR EXCLUDE ( SUBJAREA , "AGRI" ) OR EXCLUDE ( SUBJAREA , "PHAR" ) OR EXCLUDE ( SUBJAREA , "HEAL" ) OR EXCLUDE ( SUBJAREA , "EART" ) OR EXCLUDE ( SUBJAREA , "NURS" ) OR EXCLUDE ( SUBJAREA , "CHEM" ) OR EXCLUDE ( SUBJAREA , "CENG" ) OR EXCLUDE ( SUBJAREA , "VETE" ) OR EXCLUDE ( SUBJAREA , "DENT" ) ) AND ( EXCLUDE ( SUBJAREA , "COMP" ) OR EXCLUDE ( SUBJAREA , "ENGI" ) OR EXCLUDE ( SUBJAREA , "MATH" ) OR EXCLUDE ( SUBJAREA , "MEDI" )'*).

We excluded papers not written in English, that were not articles or conference papers, that were from disciplines that are likely irrelevant for the present search (e.g., Dentistry, Veterinary, Chemical Engineering, Chemistry, Nursing, Pharmacology, Microbiology, Materials Science, Medicine) or unlikely to use an experimental design (e.g. Computer Science, Engineering, Mathematics). After these filters were applied, we ended up with `r prisma$scopus` results.
Second, we conducted an advanced Google Scholar search via the "Publish or Perish" software (search string: *'"Fake news" \| "False news"\|"False stor\*" "Accuracy" \| "Discernment"\|"Credibility"\|"Belief"\|"Suceptib\*", no citations, no patents'*). The main advantage of this search was to identify important pre-prints or working papers that the Scopus search would have missed. The Google scholar search yielded `r prisma$google` results.
After removing `r prisma$duplicates` duplicates from the two searches, we ended up with `r prisma$screened` documents for screening. We screened records based on titles only. The vast majority of documents (`r prisma$screening_excluded`) had irrelevant titles and were removed during that phase. Most irrelevant titles were not about false news or misinformation (e.g. "Formation of a tourist destination image: Co-occurrence analysis of destination promotion videos"), and some were about false news or misinformation but were not about belief or accuracy (e.g. "Freedom of Expression and Misinformation Laws During the COVID-19 Pandemic and the European Court of Human Rights").
We stored the remaining `r prisma$retrieval_databases` records in the reference management system Zotero for retrieval. Of those, we rejected a total of `r prisma$exluded_databases` papers that did not meet our inclusion criteria. We rejected `r prisma$exluded_databases_abstract` papers based on their abstract and `r prisma$exluded_databases_fulltext` after assessment of the full text. We included the remaining `r prisma$retrieval_databases - prisma$exluded_databases` papers from the systematic literature search.
To complement the systematic search results, we conducted forward and backward citation search through Google Scholar. We also reviewed additional studies that we had on our computers and papers we found scrolling through twitter (mostly unpublished manuscripts). Taken together we identified an additional `r prisma$retrieval_other` papers via those methods. Of these, we excluded `r prisma$excluded_other` papers after full text assessment because they did not meet our inclusion criteria. For these papers, too, our exclusion decisions are documented in `literature_search/excluded.csv`. We included the remaining `r prisma$retrieval_other - prisma$excluded_other` papers.
In total, we included `r prisma$studies_included` papers in our meta analysis, `r descriptives$peer_review$yes$n_papers` of which were peer-reviewed and `r descriptives$peer_review$no$n_papers` grey literature (pre-prints, reports and working papers). We retrieved the relevant summary statistics directly from the paper for `r prisma$studies_included_paper` papers, calculated them ourselves based on publicly available raw data for `r prisma$studies_included_data` papers, and got them from the authors after request for `r prisma$studies_included_authors` papers.

## Statistical methods

All data and code are publicly available on the OSF. Unless explicitly mentioned otherwise, we pre-registered all reported analyses. Our choice of statistical models was informed by simulations, which can be found on the OSF, too. We conducted all analyses in R [@rcoreteam2022] using Rstudio [@positteam2023] and the `tidyverse` package [@wickham2019]. For effect size calculations, we rely on the `escalc()`, for for models on the `rma.mv()`, for clustered standard errors on the `robust()` function, all from the `metafor` package [@viechtbauer_conducting_2010].

### Deviationsfrom pre-registration

We did not pre-register considering scale symmetry as a moderator variable. We report the results regarding that variable in appendix \@ref(moderators).

### Outcomes

We have two complementary measures of assessing the quality of people's news judgement. The first measure is discernment. It measures the overall quality of news judgement across true and fake news. We calculate discernment by subtracting the mean accuracy ratings of false news from the mean accuracy ratings of true news, such that more positive scores indicate better discernment. However, discernment is a limited diagnostic of the quality of people's news judgement. Imagine a study A in which participants rate 50% of true news and 20% of fake news as accurate, and a study B finding 80% of true news and 50% of fake news rated as accurate. In both cases the discernment is the same: Participants rated true news as more accurate by 30 percentage points than fake news. However, the performance by news type is very different. In study A, people do well for fake news - they only mistakenly classify 20% as accurate - but are at chance for true news. In study B, it's the opposite. We therefor use a second measure: response bias. For any given level of discernment, it indicates whether people's judgements were better on true news or on fake news, and to which extent. First, we calculate an error for fake and true news separately, which we define as the distance of participant's actual ratings to the best possible ratings. For example, for study A, the mean error for true news is 50% (100%-50%), because in the best possible scenario, participants would have classified 100% of true news as true. The error for fake news in Study A is 20% (20%-0%), because in the best possible performance for participants would have been to classify 0% of fake news as accurate. We calculate response bias by subtracting the mean error for false news from the mean error for true news. For example, for Study A, the response bias would by (50%-20%) 30%. A positive response bias indicates that people doubt true news more than they believe fake news.

### Effect sizes

The studies in our meta analysis used a variety of response scales, including both binary (e.g. "Do you think the above headline is accurate? - Yes, No") and continuous ones (e.g. "To the best of your knowledge, how accurate is the claim in the above headline" 1 = Not at all accurate, 4 = Very accurate). To be able to compare across the different scales, we calculate standardized effects, i.e. effects expressed in units of standard deviations. Common standardized mean difference (SMD) measures such as Cohen's D or Hedge's G assume that groups (in our case fake and true news ratings) are independent. However, the vast majority of experiments (`r descriptives$design$within$n` out of `r descriptives$effects` effects) in our meta analysis manipulated news veracity within participants, i.e. having participants rate both fake and true news. To account for the dependency between ratings that this design generates, we calculated standardized mean changes using change score standardization (SMCC) [@gibbons_estimation_1993]. This change score is calculated as

$$
SMCC = \frac{MD}{SD_d}
$$

with $MD$ being the mean difference/change score (true news score minus fake news score) and $SD_d$ being standard deviation of the change scores/differences, which (assuming equal standard deviations fake and true news populations) is calculated as: $SD_d = SD_{fake/true}\sqrt{2(1-r)}$ [@morris2002].

Similar to more common standardized effect measures such as Cohen's d, the SMCC is a measure of mean difference in terms of a pooled standard deviation. The difference is that the SMCC does not assume independence between groups, by taking into account the correlation between fake and true news when calculating the pooled standard deviation.
The SMCC varies on the imputed correlation value $r$, because $SD_d$ varies as a function of $r$. If $r$ is greater than .5, $SD_d$ will be smaller than $SD_{fake/true}$, and as a result, the effect size will be larger than the one obtained by a standardized mean difference assuming independence such as Cohen's d. In contrast, when the correlation is less than .5, $SD_d$ will be greater than $SD_{fake/true}$, and an independent-groups effect size will be larger @morris2002.
Ideally, for each effect (meta-analytic observation) in our data, we need an estimate of the correlation between fake and true news ratings. However, this correlation is generally not reported in the original paper. We could only obtain it for a subset of samples for which we collected the summary statistics ourselves, based on the raw data. Based on this subset of correlations, we calculated an average correlation, which we then impute for all effects. This approach is in line with the [Cochrane recommendations for crossover trials](https://training.cochrane.org/handbook/current/chapter-23#section-23-2-7-3) [@higgins_cochrane_2019]. In our case, that average correlation is `r average_correlation %>% round(digits = 2)`. In appendix \@ref(effect-sizes) we run sensitivity analyses which show that our results hold across all correlation values that we obtain from individual-level data.
For the `r descriptives$design$between$n` (out of `r descriptives$effects`) effects from studies that used a between participant design, we calculated Hedge's g [@hedges_distribution_1981]. For all effect size calculations, we defined the sample size as the number of instances of news ratings. That is, we multiplied the number of participants with the number of news items rated per participant. 

### Models

In our models for the meta analysis, each effect - the meta-analytic units of observation - was weighted by the inverse of its standard error, thereby giving more weight to studies with larger sample sizes. We used random effects models, which assume that there is not only one true effect size but a distribution of true effect sizes [@harrer2021]. These models assume that variation in effects is not only due to sampling error alone, and thereby allow to model other sources of variance. 
We estimated the overall effect of our outcome variables using a three-level meta-analytic model with random effects on the sample and the publication level. This approach allowed us to account for the hierarchical structure of our data, in which samples (level three) contribute multiple effects (level two)^[Level 1 being the participant level of the original studies, see @harrer2021]. Multiple effects per sample occur, for example, when separate accuracy ratings are available by news topic, or when follow-up studies were conducted on the same participants. 
However, the multi-level models do not account for dependencies in sampling error. When one same sample contributes several effect sizes, we expect their respective sampling errors to be correlated [@harrer2021]. To account for dependency in sampling errors, we computed cluster-robust standard errors, confidence intervals, and statistical tests for all estimated effect sizes. 
To assess the effect of moderator variables, we calculated meta regressions. We calculated a separate regression for each moderator, by adding the moderator variable as a fixed effect to the multilevel meta models presented above. We pre-registered a list of six moderator variables to test. Those included the *country* of studies (levels: United States vs. all other countries), *political concordance* (levels: politically concordant vs. politically discordant), *news family* (levels: political, including both concordant and discordant vs. covid related vs. other, including categories as diverse as history, environment, health, science and military related news items), the *format* in which the news were presented (levels: headline only vs. headline and picture vs. headline, picture and lede), whether news items were accompanied by a *source* or not, and the *response scale* used (levels: binary vs. 4-point vs. 6-point vs. 7-point vs. other, for all other numeric scales that were not frequent). We ran an additional regression on a non-preregistered variable, namely the *symmetry of scales* (levels: perfectly symmetrical vs. imperfectly symmetrical).

### Publication bias

We ran some standard procedures for detecting publication bias. However, a priori we did not expect publication bias to be present. That is because our variables of interest were hardly ever those of interest to the researchers of the original studies: Researchers usually set out to test factors that alter discernment, and not the state of discernment in the control group. No studies measured response bias in the way we define it here.

Regarding discernment, we find evidence that smaller studies tend to report larger effect sizes according to Egger's regression test (see Fig. \@ref(fig:funnel); see also appendix \@ref(publication-bias)). However, it is unclear how meaningful this difference is. As illustrated by the funnel plot, there is generally high between-effect size heterogeneity: Even when focusing only on the most precise effect sizes (top of the funnel), the estimates vary substantially. It thus seems reasonable to assume that most of the dispersion of effect sizes does not arise from studies’ sampling error, but from studies estimating different true effects. Further, even the small studies are relatively high powered, suggesting that they would have yielded significant, publishable results even with smaller effect sizes. Lastly, Egger's regression test can lead to an inflation of false positive results when applied to standardized mean differences [@pustejovsky2019; @harrer2021]. We do not find evidence for asymmetry regarding response bias.

We do not find any evidence to suspect p-hacking from visually inspecting p-curves for both outcomes (see Fig. \@ref(fig:p-curve)).

(ref:funnel) Funnel plots for discernment and response bias. Dots represent effect sizes. In the absence of publication bias and heterogeneity, one would then expect to see the points forming a funnel shape, with the majority of the points falling inside of the pseudo-confidence region centered around the average effect estimate, with bounds of ±1.96 SE (the standard error value from the y-axis).The dashed red regression line illustrates the estimate of the Egger’s regression test. Its slope differs significantly from zero, suggesting that smaller studies systematically report larger effect sizes.

```{r, include=FALSE}
# Create funnel plot. 
funnel_raw_discernment <- viz_funnel(robust_model_accuracy, egger = TRUE,
                         xlab = "Cohen's d", text_size = 5,
                         contours_col = "Greys")

# Convert funnel plot into ggplot object so it can be ggarranged. 
funnel_discernment <- funnel_raw_discernment + ggtitle("(a) Discernment") + plot_theme +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12)) 

# Create funnel plot. 
funnel_raw_bias <- viz_funnel(robust_model_error, egger = TRUE,
                         xlab = "Cohen's d", text_size = 5,
                         contours_col = "Greys")

# Convert funnel plot into ggplot object so it can be ggarranged. 
funnel_bias <- funnel_raw_bias + ggtitle("(b) Response bias") + plot_theme +
  theme(plot.title = element_text(hjust = 0.5), 
        axis.text.x = element_text(size=12),
        axis.text.y = element_text(size=12)) 
```

```{r funnel, fig.cap="(ref:funnel)"}
ggarrange(funnel_discernment, 
          funnel_bias + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank()))

```

(ref:p-curve) P-curves for discernment and response bias. The p-curve shows the percentage of of effect sizes for a given p value within the range of 0.1 and 0.5. All values smaller than 0.01 are rounded to that value. The reference lines indicate the expected percentage of studies for a given p value, assuming that there is a true effect and certain statistical power to detect it (either 0% or 30% power). The observed p-curve is negatively sloped and heavily right skewed (the tail points to the right) for both outcomes, which suggests no widespread p-hacking.

```{r, include=FALSE}
pcurve_discernment <- plot_pcurve(accuracy_effect)
pcurve_bias <- plot_pcurve(error_effect)
```

```{r p-curve, fig.cap="(ref:p-curve)"}
ggarrange(pcurve_discernment, 
          pcurve_bias + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank(),
                  axis.title.y = element_blank()))
```

### Data availability

The extracted data used to produce our results are available at [OSF link].

### Code availability

The code used to create all results (including tables and figures) of this manuscript is available at [OSF link].

# References

<div id="refs"></div>

\newpage

# (APPENDIX) Appendix {-}

```{r child = "appendix_a.Rmd"}
```

```{r child = "appendix_b.Rmd"}
```

```{r child = "appendix_c.Rmd"}
```

```{r child = "appendix_d.Rmd"}
```

```{r child = "appendix_e.Rmd"}
```

```{r child = "appendix_f.Rmd"}
```

```{r child = "appendix_g.Rmd"}
```

