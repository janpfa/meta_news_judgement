# Selection bias {#selection-bias}

Skepticism bias could be an artifact of biased news selection for experiments. For example, one might suspect researchers to pick easy-to-detect false news and/or hard-to-detect true news (e.g. to avoid ceiling effects), thus inflating participants' skepticism of true news.

We deem this unlikely because of several reasons: First, while ceiling effects may have biased the selection of 'non-obviously true' true news, floor effects should have conversely biased the selection of 'non-obviously false' false news. We see no reason to suspect that news selection has created accuracy asymmetries between false and true news items. Researchers did sometimes pre-test items to avoid using excessively ambiguous headlines. Yet, no article mentions asymmetries in this relatively rare selection process.

Second, degrees of accuracy could be captured by Likert scales, but not by binary scales. If a selection bias is mild, i.e. leading to nuances within veracity categories but not to misclassification (e.g. people rating a true item as 5 instead of 7 on a 7-point accuracy scale, but not as a 3), then collapsing responses from Likert scales to binary outcomes should account for it. In Appendix \@ref(binary), we show that our results hold when doing so for the subset of studies for which we have individual-level data.

```{r}
# popularity
# Third, if selection bias is driven by popularity of news, our conclusions would still hold. One could argue that selected false news were popular, whereas true news weren’t. As a result, the false news could have been easier to classify, inflating the skepticism  bias. However, if this would have indeed inflated the skepticism  bias, it would still suggest that (i) people know popular false news to be false,(ii) people tend to reject new information by default, or (iii) both.
```

```{r}
# calculate scaled version of Stewart et al. (2021)
stewart <- (5.05 - 1)  / (7 - 1)

# calculate scaled version of Hohenberg
hohenberg <- meta_wide %>% 
  select(ref, mean_accuracy_true, sd_accuracy_true, accuracy_scale_numeric) %>% 
  filter(ref == "Hohenberg_2023") %>% 
  mutate(scaled_mean = (mean_accuracy_true - 1) / (accuracy_scale_numeric - 1), 
         scaled_sd_accuracy_true = sd_accuracy_true / (accuracy_scale_numeric - 1)) %>% 
  round_numbers()

# calculate scaled version of Hohenberg
garrett <- meta_wide %>% 
  select(ref, mean_accuracy_true, sd_accuracy_true, accuracy_scale_numeric) %>% 
  filter(ref == "Garrett_2021") %>% 
  mutate(scaled_mean = (mean_accuracy_true - 1) / (accuracy_scale_numeric - 1), 
         scaled_sd_accuracy_true = sd_accuracy_true / (accuracy_scale_numeric - 1)) %>% 
  summarise(
    accuracy_scale_numeric = mean(accuracy_scale_numeric),
    mean_accuracy_true = mean(mean_accuracy_true),
    sd_accuracy_true = mean(sd_accuracy_true),
    scaled_mean = mean(scaled_mean), 
    scaled_sd_accuracy_true = mean(scaled_sd_accuracy_true)) %>% 
  round_numbers()
  
# check effects of Luo and Hohenberg studies
# bind_rows(
# error_effect %>% 
#   mutate(conf_low = yi - 1.96*sqrt(vi), 
#          conf_high = yi + 1.96*sqrt(vi), 
#          outcome = "error") %>% 
#   select(ref, unique_sample_id, yi, conf_low, conf_high, outcome) %>% 
#   filter(grepl("Luo", ref) | grepl("Hohenberg", ref)), 
# accuracy_effect %>% 
#   mutate(conf_low = yi - 1.96*sqrt(vi), 
#          conf_high = yi + 1.96*sqrt(vi), 
#          outcome = "accuracy") %>% 
#   select(ref, unique_sample_id, yi, conf_low, conf_high, outcome) %>% 
#   filter(grepl("Luo", ref) | grepl("Hohenberg", ref))
# )
```

Third, and most importantly, we observe similar average accuracy ratings for true news in four studies that randomly sampled true news from high-quality mainstream news sites. These samples of headlines are free of any selection bias that may originate from researchers selecting not obviously accurate true headlines. @stewartDistortingEffectsProducer2021 used CrowdTangle to automatically scrap 500 headlines from 20 mainstream news sites and had participants rate the accuracy of these headlines. The mean accuracy rating of these headlines was 5.05 (sd = 0.56) on a 7-point scale, or `r stewart` if we transpose the scale to reach from 0 to 1. This is similar to our (unweighed) average true news rating (`r  descriptives$data_descriptive_plot$true$mean_value`) when scaling effect sizes to range from 0 to 1 (see Fig. \@ref(fig:descriptive)). Similarly, @clemmvonhohenbergTruthBiasLeft2023 automatically scrapped true headlines using the Google News API. On a 7-point scale, the average true news rating was `r hohenberg$mean_accuracy_true` (sd = `r hohenberg$sd_accuracy_true`), or `r hohenberg$scaled_mean` on a scale from 0 to 1. In a panel study over six months, @garrettConservativesSusceptibilityPolitical2021 used the NewsWhip API to automatically scrap timely news headlines, selecting the most popular ones on social media. On a 4-point scale, the average true news rating was `r garrett$mean_accuracy_true` (sd = `r garrett$sd_accuracy_true`), or `r garrett$scaled_mean` on a scale from 0 to 1. Similarly, @shirikovFakeNewsAll2023 used web scraping to automatically download top news stories on politics and international news from Yandex News (Russia's largest news aggregator). They found that true news stories selected with this process (items 31 to 49 in Study 2) were rated as true 58.76% of the time, yielding a slightly lower average true news rating compared to the one in the meta-analysis.
```{r}
# note that the numbers for 
# Shirikov, A. (2023). Fake News for All: How Citizens Discern Disinformation in Autocracies. Political Communication, 0(0), 1–21. https://doi.org/10.1080/10584609.2023.2257618
# are calculated manually as an average from the respective headlines in the appendix
```

Finally, while the four studies detailed above suggest that our results for skepticism bias (H2) are not caused by a selection bias of true news, two studies (@aslettEcologicallyExternallyValid2023 and @garrettConservativesSusceptibilityPolitical2021) suggest that it may be due to a selection bias of false news. In a recent working paper, @aslettEcologicallyExternallyValid2023 automatically scrapped popular headlines on reliable and unreliable websites in the US over a period of one month. They found that participants discerned between true and false/misleading headlines, but that they were better at rating true headlines as true than false/misleading headlines as false/misleading (suggesting a negative skepticism bias, i.e. a credulity bias). Their results converge with a large panel study included in our meta-analysis by @garrettConservativesSusceptibilityPolitical2021. They also relied on automatically scrapped popular headlines: As shown in table \@ref(tab:garrett-table), they also found a negative skepticism bias (i.e. a credulity bias). A moderator analysis suggests that this negative response bias is at least partially driven by of political concordance. Contrary to the findings in our meta-analysis (including data from @garrettConservativesSusceptibilityPolitical2021), their participants showed a strong tendency towards credulity when news headlines were concordant with their political stance, while only being slightly credulous when facing politically discordant headlines.

```{r}
# There might be an argument to make here, but not yet clear enough

# Which result - the positive, skeptical skepticism  bias of our meta-analysis or the negative, gullible skepticism  bias of @aslettEcologicallyExternallyValid2023 - deserves more attention? This depends on the population of false news that we want to draw inferences on. While we can only speculate, we think that the sample of false news of @aslettEcologicallyExternallyValid2023 is more representative of the population of false news as they get published (at least in a US context). However, there is a chance that the sample of false news in our meta analysis is more representative of the population of false news as they get consumed. This distinction between production and consumption matters, because the average person is not much exposed to the hyperpartisan and fringe media outlets, but mostly mainstream media (REF). It seems therefore reasonable to assume that the average person encounters only exceptionally popular false news, and probably at a stage where they are already debunked by fact-checking websites.
```

```{r garrett-table}
# Calculate results for Garrett study
garrett <- meta_wide %>% 
  filter(ref == "Garrett_2021")

# calculate main effect sizes (Cohen's d, following Cochrane) 
garrett_accuracy_effect <- calculate_effect_sizes(effect = "accuracy", measure = "Cochrane", 
                                          data = garrett)
garrett_error_effect <- calculate_effect_sizes(effect = "error", measure = "Cochrane", 
                                       data = garrett)

# Models using Cohen's d
garrett_model_accuracy <- calculate_models(data=garrett_accuracy_effect, robust = TRUE)
garrett_model_error <- calculate_models(data=garrett_error_effect, robust = TRUE)

# make a plot for political concordance 
# calculate models
garrett_concordance_accuracy <- metafor::rma.mv(yi, 
                                                vi, 
                                                mods = ~political_concordance,
                                                data=garrett_accuracy_effect)
garrett_concordance_error <- metafor::rma.mv(yi, 
                                                vi, 
                                                mods = ~political_concordance,
                                                data=garrett_error_effect)

# Results table
modelsummary::modelsummary(list("Discernment" = garrett_model_accuracy, 
                                "Skepticism  bias" = garrett_model_error,
                                "Discernment" = garrett_concordance_accuracy, 
                                "Skepticism  bias" = garrett_concordance_error,
                                "Discernment" = robust_model_accuracy, 
                                "Skepticism  bias" = robust_model_error),
                           title = 'Model results', 
                           stars = TRUE, 
                           coef_rename = c("overall" = "Estimate (intercept)", 
                                           "intercept" = "Estimate (intercept)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)")
                           ) %>%
  add_header_above(c(" " = 1, "Garrett & Bond, 2021" = 4, "Main results" = 2)) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down") %>%
  footnote(general = "Results from a meta-analysis of the panel study by Garrett & Bond 2021. The results for the moderator analysis for political concordance are based on less observations than the overal analysis, because the latter includes politically neutral headlines and participants who did identify as neither democrat nor republican. For reference, we included the mains results from the meta-analysis (including the study by Garrett and Bond)."
           , threeparttable = TRUE)
```

```{r}
# get predictions for skepticism bias from moderator results
intercept <- coef(garrett_concordance_error)[[1]]

garrett_predictions <- garrett_concordance_error %>% 
  tidy(conf.int=TRUE) %>% 
  mutate(
    political_concordance = ifelse(term == "intercept", "concordant", "discordant"), 
    # make sure to add the value of the intercept to have the predicted value
    # instead of the differential
    across(c(estimate, conf.low, conf.high), ~ifelse(term == "intercept", .x, 
                                                     .x + intercept), 
           .names = "predicted_{.col}")
  )
```

We believe that the discrepancies between the findings of @aslettEcologicallyExternallyValid2023 and @garrettConservativesSusceptibilityPolitical2021 on the one side and the findings of our meta-analysis on the other, boil down to the sample of false news. The papers included in our meta-analysis almost exclusively rely on false news identified as such by fact-checking websites, whereas @aslettEcologicallyExternallyValid2023 hired a team of fact checkers to verify news items and @garrettConservativesSusceptibilityPolitical2021 verified news items themselves. Thus, many false news included in @aslettEcologicallyExternallyValid2023 and @garrettConservativesSusceptibilityPolitical2021 may never appear on fact-checking websites. If that’s the case, and that fact-checking websites tend to focus on news stories that are easier to fact-check, then it could explain why participants were worse at detecting false news in these studies. Note that it is unlikely that this difference is due to the timing of the publication: @aslettEcologicallyExternallyValid2023 found that participants were better at decking false news within 48 hours of publication compared to 3 months or more after.
