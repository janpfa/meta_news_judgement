# Moderators {#moderators}

\FloatBarrier  

All moderator analyses, with the exception of political concordance, only reveal statistical associations, not causal effects, because the moderator variables vary mostly between studies: For example, some studies provided news sources, while others did not. But these studies differ in many other ways, all of which potentially confound any observed association.

Table \@ref(tab:moderators-discernment-table) shows the results of the different meta regressions by moderator variable on discernment and Table \@ref(tab:moderators-bias-table) on response bias. Figures \@ref(fig:moderators-discernment) and \@ref(fig:moderators-bias) visualize those results by showing the distribution of effect sizes by moderator variable.

```{r moderators-discernment-table}
# display models
modelsummary::modelsummary(moderator_models_accuracy,
                           title = 'Moderator effects on Discernment',
                           stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)"
                                           )
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```

```{r moderators-bias-table}
# display models
modelsummary::modelsummary(moderator_models_error,
                           title = 'Moderator effects on Response bias',
                           stars = TRUE,
                           coef_rename = c("country_groupedUS" = "Country: US (vs. nonUS)",
                                           "political_concordancediscordant" = "Political Concordance : Discordant (vs. Concordant)",
                                           "news_family_groupedother" = "News family: Other (vs. Covid)",
                                           "news_family_groupedpolitical" = "News family: Political (vs. Covid)",
                                           "news_format_groupedheadline_picture" = "News Format: Headline & Picture (vs. Headline)",
                                           "news_format_groupedheadline_picture_lede" = "News Format: Headline, Picture & Lede (vs. Headline)",
                                           "news_sourceTRUE" = "News source: Source (vs. No source)",
                                           "accuracy_scale_grouped6" = "Accuracy Scale: 6 (vs. 4)",
                                           "accuracy_scale_grouped7" = "Accuracy Scale: 7 (vs. 4)",
                                           "accuracy_scale_groupedbinary" = "Accuracy Scale: binary (vs. 4)",
                                           "accuracy_scale_groupedother" = "Accuracy Scale: other (vs. 4)",
                                           "perfect_symetryTRUE" = "Symmetrie: perfect (vs. imperfect)"
                                           )
                           ) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```

(ref:moderators-discernment) Distribution of effect sizes for discernment by moderator variables.

```{r moderators-discernment, fig.height= 10, fig.cap="(ref:moderators-discernment)"}
moderator_plots(accuracy_effect, "Discernment")
```

(ref:moderators-bias) Distribution of effect sizes for response bias by moderator variables.

```{r moderators-bias, fig.height= 10, fig.cap="(ref:moderators-bias)"}
moderator_plots(error_effect, "Response bias")
```

### Not preregistered moderators

#### Scale symmetry

First, to avoid biasing our estimate for H2, we removed one study [@baptistaInfluencePoliticalIdeology2021] that used a very asymmetrical set of answer options asked participants ("According to your knowledge, how do you rate the following headline? 1---not credible; 2---somehow credible; 3---quite credible; 4---credible; 5---very credible"). Second, we coded whether the remaining scales were perfectly symmetrical or not. Table \@ref(tab:n-symmetry) shows the frequency by which both scale types occurred.

```{r n-symmetry}
meta_wide %>% group_by(perfect_symetry) %>% summarise(Papers = n_distinct(paperID),
                                                     Samples = n_distinct(unique_sample_id),
                                                     Effects = n_distinct(observation_id)
                                                     ) %>% 
  mutate(perfect_symetry = ifelse(perfect_symetry == TRUE, "Perfect symmetry", 
                                 "Imperfect Symmetry")) %>% 
  pivot_longer(-perfect_symetry,
               names_to = "variable",
               values_to = "frequency"
               ) %>% 
  pivot_wider(names_from = perfect_symetry, 
              values_from = frequency) %>% 
  column_to_rownames(var = "variable") %>% 
  apa_table(note = "Frequency table of scales.")
```

Perfectly symmetrical scales include all binary scales (e.g. "True" or "False", "Real" or "Fake", is accurate "Yes" or "No", is accurate and unbiased "Yes" or "No"), and most Likert-scales (1 to 7: "Definitely fake" [1] to "Definitely real" [7], "Very unreliable" [1] to "Very reliable" [7], "Extremely unlikely" [1] to "Extremely likely" [7], "Extremely unbelievable" [1] to "Extremely believable" [7]; 1 to 6: "Extremely inaccurate" [1] to "Extremely accurate" [6], "Completely false" [1] to "Completely true" [6]). Yet, we coded the most common scale, a 4-point Likert scale ([1] not at all accurate, [2] not very accurate, [3] somewhat accurate, [4] very accurate), as not perfectly symmetrical. We coded two other Likert scales as not perfectly symmetrical ("not at all trustworthy" [1] to "very trustworthy" [10]; "not at all" [1] to "very" [7]).

Third, we investigated whether H1 and H2 hold for both perfectly symmetrical and imperfectly symmetrical scales. While both H1 and H2 hold for both symmetry types, we found that studies with perfectly symmetric scales tend to yield lower discernment scores ($\Delta$ Discernment = `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$estimate` `r list_moderators_accuracy$Symmetrie$perfect_symetryTRUE$ci`) than studies relying on scales that are at least slightly asymmetric (Baseline discernment slightly asymmetric scales = `r list_moderators_accuracy$Symmetrie$intercept$estimate` `r list_moderators_accuracy$Symmetrie$intercept$ci`). We do not find a difference regarding response bias.

The results suggest that imperfectly symmetrical scales may inflate discernment. However, the symmetry of response scales was not a factor that was experimentally manipulated, and the studies we compare in our model differ in many other ways and the observed difference is likely confounded.

#### Proportion of true news

Most studies exposed participants to 50% of false news and 50% of true news, whereas outside of experimental settings, people on average are exposed to much more true news than false news [@altayQuantifyingInfodemicPeople2022]. This inflated proportion of false news may increase discernment or make participants more skeptical of true news. Empirical evidence suggests that the ratio of false news has no effect on discernment and slightly increases skepticism in news judgment [@altayExposureHigherRates2023]. Figure \@ref(fig:share-true) shows effect sizes for discernment and response bias as a function of news ratio. Due to the very uneven number of effect sizes, it does not seem reasonable to run a meta-regression to test this. However, Fig. \@ref(fig:share-true) suggests no obvious trend with regard to the share of true news ratio. Besides, as for the other moderator variables, any observed association is likely to be confounded by other factors.

(ref:share-true) Effect sizes plotted by their share of true news among all news that an individual participant saw.

```{r share-true, fig.cap="(ref:share-true)"}
share_true_discernment <- plot_share_true(data = accuracy_effect, name = "Discernment")
share_true_bias <- plot_share_true(data = error_effect, name = "Response bias")

ggarrange(share_true_discernment, 
          share_true_bias  + 
            theme(axis.text.y = element_blank(),
                  axis.ticks.y = element_blank()))

```
