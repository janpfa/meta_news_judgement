# Individual level data {#individual-level}

\FloatBarrier  

We compare the results of our main meta model to the individual-level data with the following procedure: First, we restrict our data to (i) only studies using a non-binary response scale and (ii) only those studies that we have individual-level data on. Second, we run the same meta-analytic model as in the main analysis on the effect sizes of that subset of studies. Third, we take the individual-level data of that subset of studies and run a mixed model on it.

The meta-model estimates are standardized. To be able to compare results, we standardized participants' accuracy ratings in the individual-level data as follows: Within each sample, we calculated the standard deviation of accuracy ratings (false and true news combined). Then, for each sample, we divided accuracy ratings by the respective standard deviation.

We use the `lme4` package [@batesFittingLinearMixedEffects2015] and its `lmer()` function to run the mixed models. The mixed models include random effects by participant (each participant provides several ratings for both true and false news) and by sample for both the intercept and the effect of veracity. In our models, participants are nested in samples.

As shown in Fig. \@ref(fig:individual-vs-meta), this individual-level analysis yields an estimate very similar to our meta-analytic average. 

```{r individual-data, message=FALSE}
# load data
individual_level_subset <- read_csv("data/individual_level_subset.csv")

# compute standardized accuracy and error measures 
individual_level_subset <- individual_level_subset %>% 
  # remove binary scales and treatment conditions
  filter(scale != "binary" & condition == "control") %>% 
  # remove NA's for accuracy
  drop_na(accuracy) %>% 
  # identify unique samples
  mutate(unique_sample_id = paste(paper_id, sample_id, sep = "_"),
         # make an ungrouped versions just for comparison
         ungrouped_std_acc = accuracy/sd(accuracy), 
         ungrouped_std_err = error/sd(accuracy)
         ) %>% 
  # group by unique samples
  group_by(unique_sample_id) %>% 
  # calculate standardized accuracy measure
  mutate(accuracy_std = accuracy/sd(accuracy), 
         error_std = error/sd(accuracy)) %>% 
  # ungroup
  ungroup()
```

```{r reduce-data}
# These steps are to identify those studies that we have raw data on in the 
# meta data and subset that data accordingly. 

# Step 1: extract `unique_sample_id` for samples in raw data
raw_data_samples <- individual_level_subset %>%
  reframe(unique(unique_sample_id)) %>% 
  pull()

# Step 2: filter the meta data frame
# We want to reduce our data to continuous measure samples that we have raw data on
reduce_samples <- function(data) {
  
    results <- data %>% 
      filter(unique_sample_id %in% all_of(raw_data_samples))
}
# for accuracy
reduced_accuracy_effect <- reduce_samples(accuracy_effect)
# for error
reduced_error_effect <- reduce_samples(error_effect)
```

```{r individual-models}
# Run linear mixed model with random slope and intercept for subject id nested
# in sample id

# Discernment
individual_level_accuracy <- lmer(accuracy_std ~ 1 + veracity + 
                                 (1 + veracity | unique_sample_id / id),
                data = individual_level_subset) %>% 
  tidy(conf.int = TRUE) %>% 
  mutate(model = "individual level", 
         outcome = "accuracy") %>% 
  # select only effect of interest
  filter(term == "veracitytrue")

# Response bias
individual_level_error <- lmer(error_std ~ 1 + veracity + 
                                 (1 + veracity | unique_sample_id / id),
                data = individual_level_subset) %>% 
  tidy(conf.int = TRUE) %>% 
  mutate(model = "individual level", 
         outcome = "error") %>% 
  # select only effect of interest
  filter(term == "veracitytrue") 
```

```{r reduced-meta-models}
# Run the meta model on reduced data 
reduced_accuracy <- robust(metafor::rma.mv(yi, vi, 
                                  random = ~ 1 | unique_sample_id / 
                                    observation_id, data = reduced_accuracy_effect ),
                  cluster = reduced_accuracy_effect$unique_sample_id
) %>% 
  tidy(conf.int = TRUE) %>% 
  mutate(model = "reduced meta", 
         outcome = "accuracy")

reduced_error <- robust(metafor::rma.mv(yi, vi, 
                                  random = ~ 1 | unique_sample_id / 
                                    observation_id, data = reduced_error_effect ),
                  cluster = reduced_error_effect$unique_sample_id
) %>% 
  tidy(conf.int = TRUE) %>% 
  mutate(model = "reduced meta", 
         outcome = "error")
```

(ref:individual-vs-meta) Comparison of meta to individual level analysis (continuous scales only). "Meta" corresponds to the main results reported in the paper; "meta reduced" are the same meta-analytic models as in the main analysis but run on the subset of studies for which we have individual level data; "individual-level" corresponds to the result of mixed effect models run on the individual-level data. Symbols represent estimates, horizontal bars 95% confidence intervals. 

```{r individual-vs-meta, fig.cap="(ref:individual-vs-meta)"}
# We store the results of all analyses in a single data frame
comparison <- bind_rows(
  # models individual level data 
  individual_level_accuracy, 
  individual_level_error,
  # models on reduced data
  reduced_accuracy, 
  reduced_error,
  # main models
  robust_model_accuracy %>% 
    tidy(conf.int = TRUE) %>% 
    mutate(model = "meta", 
           outcome = "accuracy"), 
  robust_model_error %>% 
    tidy(conf.int = TRUE) %>% 
    mutate(model = "meta", 
           outcome = "error")) %>% 
  round_numbers() %>% 
  mutate(
    # give a nicer name to the estimate
    term = "estimate", 
    # extract confidence intervals
    ci = glue::glue("[{conf.low}, {conf.high}]"),
    # Change outcome names
    outcome = ifelse(outcome == "accuracy", "Discernment", "Response bias")
  )

# plot results
ggplot(comparison,
       aes(x = estimate, y = model, shape = model, linetype = model)) +
  geom_vline(xintercept = 0, linewidth = 0.5, linetype = "24", color = "grey") +
  geom_pointrange(aes(xmin = conf.low, xmax = conf.high, color = model),
                  position = position_dodge(width = -0.6)) +
    # Add geom_text layer
  geom_text(aes(label = paste0(estimate," ", ci)), position = position_dodge(), vjust = -1.2) + 
  labs(x = "Standardized effect estimate", y = NULL, linetype = NULL,
       shape = NULL, color = NULL) +
  scale_color_viridis_d(option = "plasma", end = 0.9) +
  scale_x_continuous(limits = c(0, 1.25)) +
  plot_theme +
  theme(legend.position = "bottom",
        axis.text.y = element_blank(),
        strip.text = element_text(size = 14)) +
  facet_wrap(~outcome)

```

## How skilled were individual participants?

In our meta analysis, we find that people discern well between true and false news - on average. But how skilled are individuals in discerning true from false?

We calculate a discernment score per individual for all individual level studies. To compare across different scales, we transpose all accuracy scores on a scale from 0 to 1, resulting in a range of possible values from -1 to 1 for both discernment and response bias (see \@ref(fig:individuals-distribution)). 

(ref:individuals-distribution) Distribution of average discernment and response bias scores per individual participant in the subset of studies that we have raw data on. We standardized original accuracy ratings to range from 0 to 1, to be able to compare across scales. Therefore, the worst possible score is -1 where, for discernment, an individual classified all news wrongly, and for response bias, an individual classified all true news correctly (as true) and all false news incorrectly (as true). The best possible score is 1 where, for discernment, an individual classified all news correctly, and for response bias, an individual classified all true news incorrectly (as false) and all false news correctly (as false).

```{r}
# load data
individual_level_subset <- read_csv("data/individual_level_subset.csv")

# compute standardized accuracy and error measures 
outcomes_by_participant <- individual_level_subset %>% 
  # remove binary scales and treatment conditions
  filter(condition == "control") %>% 
  # remove NA's for accuracy
  drop_na(accuracy) %>% 
  # identify unique participants
  mutate(unique_participant_id = paste(paper_id, sample_id, id, sep = "_")) %>% 
  mutate(
      # standardize_accuracy
      across(c(accuracy), 
             ~ifelse(
               # Binary and 0 to 1 scale are already on the wanted scale
               scale == "binary" |
                 scale == "1", .x, 
               # for all other numeric scales
               (.x-1)  / (scale_numeric - 1)), 
             .names = "std_{col}"),
      # standardize_error
      across(c(error), 
             ~ifelse(
               # Binary and 0 to 1 scale are already on the wanted scale
               scale == "binary" |
                 scale == "1", .x, 
               # for all other numeric scales
               .x  / (scale_numeric - 1)), 
             .names = "std_{col}"), 
  ) %>% 
  # calculate averages scores per participant and veracity
  group_by(unique_participant_id, veracity) %>%
  summarize(mean_std_accuracy = mean(std_accuracy), 
         mean_std_error = mean(std_error),
  ) %>% 
  # turn data frame into wide format to be able to calculate discernment
  pivot_wider(names_from = veracity, 
              values_from = c(mean_std_accuracy, mean_std_error)) %>% 
  # calculate discernment and response bias
  mutate(discernment = mean_std_accuracy_true - mean_std_accuracy_fake, 
         response_bias = mean_std_error_true - mean_std_error_fake) %>% 
  ungroup()
```

```{r individuals-distribution, fig.cap="(ref:individuals-distribution)"}
# plot

# shape data to long format
data <- outcomes_by_participant %>% 
  pivot_longer(c(discernment, response_bias),
               names_to = "outcome", 
               values_to = "value") %>% 
  # make nicer names
  mutate(outcome = ifelse(outcome == "discernment", "Discernment", 
                          "Response bias"))

# make plot
ggplot(data, aes(x = value, fill = outcome, color = outcome)) +
  geom_density(alpha = 0.5)+
  # add line at 0
  geom_vline(xintercept = 0, 
             linewidth = 0.5, linetype = "24", color = "grey") +
  # scale
  scale_x_continuous(breaks = seq(from = -1, to = 1, by = 0.2)) +
  # colors 
  scale_color_viridis_d(option = "turbo", begin = 0.25, end = 1)+
  scale_fill_viridis_d(option = "turbo", begin = 0.25, end = 1) +
  # labels and scales
  labs(x = "Standardized scores \n (scale from -1 to 1)", y = "Density") +
  guides(fill = FALSE, color = FALSE) +
  plot_theme +
  theme(legend.position = "bottom",
        axis.text.y = element_blank(),
        strip.text = element_text(size = 14)) +
  facet_wrap(~outcome)
```

We report the absolute number of individuals with a positive vs. negative discernment and response bias score in Table \@ref(tab:individuals-direction). 

```{r individuals-direction}
# table 
data %>% 
  drop_na(value) %>% 
  mutate(value = ifelse(value > 0, "positive", "negative")) %>% 
  group_by(value, outcome) %>% 
  summarize(n_subj = n_distinct(unique_participant_id)) %>% 
    pivot_wider(names_from = outcome, 
              values_from = n_subj) %>% 
  # relative frequency
  ungroup() %>% 
  mutate(
    rel_discernment = Discernment / sum(Discernment),
    rel_response_bias = `Response bias` / sum(`Response bias`)
    ) %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  # add brackets around relative values
  mutate(across(c(rel_discernment, rel_response_bias), ~paste0("(", .x, ")"))) %>% 
      # unite absolute and relative values in the same variable
  mutate(Discernment = paste0(Discernment, " ", rel_discernment), 
         `Response bias` = paste0(`Response bias`, " ", rel_response_bias)
         ) %>% 
  select(value, Discernment, `Response bias`) %>% 
  column_to_rownames(var = "value") %>% 
  apa_table(note = "Frequency table of total number of participants that had a positive or negative score for both outcomes.")
```


