# Binary vs. continuous scales {#binary}

\FloatBarrier  

Some of the studies included in our review measure perceived accuracy on a continuous scale, others on a binary (or dichotomous) scale. This is not problematic per se - there are [statistical methods to compare effects on both scales](https://training.cochrane.org/handbook/current/chapter-10#section-10-6) [@higgins_cochrane_2019]. These require, however, appropriate summary statistics for both scales. For continuous measures, means and standard deviations are fine; for binary measures we would need, for example, odds or risk ratios. The problem we were facing is that authors did not provide the appropriate summary statistics for binary scales. Instead, they tended to report means and standard deviations, just as they do for continuous outcomes. For the main analysis, we made the decision to treat continuous and binary scales in the same way, glossing over potential biases from inappropriate summary statistics. 
Here, we include robustness checks to see how this decision affects our results. First, we ran meta-regression to see if there are any differences regarding our outcome variables associated with type of scale (continuous vs. binary). This analysis suggests that there is a difference regarding skepticism  bias between studies using binary and continuous scales (smaller bias for binary scales). Second, we focus on the subset of studies that we have individual-level data on. For this subset, we combine binary and continuous response scales by collapsing the latter on a binary outcome. We calculate appropriate effect sizes for binary data, namely log odds ratios (logORs) and run a meta-analysis on those effect sizes. The results are in line with our main findings, suggesting positive discernment and skepticism  bias. 

## Meta-regression

We ran a meta-regression using scale type (two levels: binary vs. continuous) as a predictor variable. Table \@ref(tab:binary-vs-continuous-regression) summarizes the results, and Fig. \@ref(fig:binary-continuous-descriptive) illustrates them. The analysis suggests that skepticism  bias is more enhanced among continuous studies. However, we cannot tell how much of that observed difference is due to relying on imperfect summary statistics for binary scale, or due to other factors.

```{r}
# Start with making a new variable distinguishing between binary and 
# continuous scales. 
add_binary_continuous_scales <- function(data) {
  
  results <- data %>% 
    mutate(scale_binary_continuous = ifelse(accuracy_scale == "binary", "binary", 
                                            "continuous")
    )
}

accuracy_effect <- add_binary_continuous_scales(accuracy_effect)
error_effect <- add_binary_continuous_scales(error_effect)
```

(ref:binary-continuous-descriptive) Distribution of effect sizes (Cohen's d) grouped by whether a binary or continuous response scale was used.

```{r binary-continuous-descriptive, fig.cap="(ref:binary-continuous-descriptive)"}
data <- bind_rows(accuracy_effect %>% mutate(outcome = "accuracy"), 
                  error_effect %>% mutate(outcome = "error")) %>% 
  # Change outcome names
  mutate(outcome = ifelse(outcome == "accuracy", "Discernment", "Skepticism  bias"))

ggplot(data,
       aes(x = scale_binary_continuous, y = yi)) +
    geom_half_boxplot(aes(x = scale_binary_continuous, color = scale_binary_continuous), side = "l", size = 0.5, nudge = 0.05, 
                      outlier.shape = NA) +
    geom_half_violin(aes(fill = scale_binary_continuous), side = "r") +
    geom_half_point(aes(color = scale_binary_continuous), side = "l", 
                    transformation_params = list(height = 0, width = 0.1, seed = 1)) +
    # add line of 0
    geom_hline(yintercept = 0, 
               linewidth = 0.5, linetype = "24", color = "grey") +
    # colors 
    scale_color_viridis_d(option = "plasma", end = 0.9)+
    scale_fill_viridis_d(option = "plasma", end = 0.9) +
    # labels and scales
    labs(x = NULL, y = "Cohen's d", fill = NULL) +
  guides(color = FALSE, fill = FALSE) +
    plot_theme +
    coord_flip() +
  plot_theme +
    theme(strip.text = element_text(size = 14)) +
    facet_wrap(~outcome)
```

```{r}
# function that calculates a meta regression and takes a data frame
# as input
scale_comparison_meta_regression <- function (data, return_as = "model") {
  
  model <- robust(metafor::rma.mv(yi, vi, 
                                  mods = ~scale_binary_continuous,
                                  random = ~ 1 | unique_sample_id / 
                                    observation_id, data = data),
                  cluster = data$unique_sample_id
  ) 
  
  if (return_as == "model") {
    return(model)
  }
  
  if(return_as == "tidy") {
    
    model %>% 
      tidy(conf.int = TRUE) %>% 
      mutate(model = "Original SMDs (all data)") %>% 
      # give a nicer name to the estimate
      mutate(term = ifelse(term == "scale_binary_continuouscontinuous", 
                           "effect of continuous scale (baseline binary)", term)
      )
    
    return(tidy) 
  }
}

# calculate the model for accuracy
scales_comparison_all_data_accuracy <- scale_comparison_meta_regression(accuracy_effect, return_as = "tidy")
# for error
scales_comparison_all_data_error <- scale_comparison_meta_regression(error_effect, return_as = "tidy")

# discernment
scales_comparison_discernment <- scale_comparison_meta_regression(accuracy_effect)

# skepticism  bias
scales_comparison_bias <- scale_comparison_meta_regression(error_effect)
```

```{r binary-vs-continuous-regression}
# main result table
modelsummary::modelsummary(list("Discernment" = scales_comparison_discernment, 
                                "Skepticism  bias" = scales_comparison_bias),
                           title = 'Model results', 
                           stars = TRUE, 
                           coef_rename = c("scale_binary_continuouscontinuous" = "Continuous (vs. binary)")
                           )
```

## Individual-level data

Here, we focus on the subset of studies that we have raw, individual-level data on. For this subset of studies, we first calculated the odds ratios from the raw data^[A general overview of appropriate summary statistics for binary outcomes can be found here(@higgins_cochrane_2019): https://training.cochrane.org/handbook/current/chapter-06#section-6-4)]. We then ran a meta-analysis on the odds ratios.

```{r, message=FALSE}
# Step 1: extract `unique_sample_id` for samples with binary outcomes that 
# raw data is available for 

# load data (again, since we previously did some modifications)
individual_level_subset <- read_csv("data/individual_level_subset.csv")

binary_samples_raw_data <- individual_level_subset %>% 
  filter(scale == "binary") %>% 
  # (note: do not pick `paper_id` because within a paper, some samples might have
  # been measured on binary, others on a continuous scale)
  summarize(unique(unique_sample_id)) %>% 
  pull()

# Step 2: filter the meta data frame
# We want to reduce our data to continuous measure samples AND only those binary 
# measure samples that we have raw data on

reduce_binary_samples <- function(data) {
  
    results <- data %>% 
      filter(accuracy_scale != "binary" | unique_sample_id %in% all_of(binary_samples_raw_data))
}
# for accuracy
reduced_accuracy_effect <- reduce_binary_samples(accuracy_effect)
# for error
reduced_error_effect <- reduce_binary_samples(error_effect)

```

### Odds ratios 

The 'odds' refer to the ratio of the probability that a particular event will occur to the probability that it will not occur, and can be any number between zero and infinity [@higgins_cochrane_2019]. It is commonly expressed as a ratio of two integers. For example, in a clinical context, 1 out of 100 patients might die; then the odds of dying are `0.01`, or `1:100`.

The odds *ratio* (OR) is the ratio of the Odds. The odds ratio that characterizes discernment is calculated as

$$
OR_{Accuracy} = \frac{(Accurate_{true}/ NotAccurate_{true})}{(Accurate_{false}/ NotAccurate_{false})}
$$

If the OR is `1`, participants were just as likely to rate items as 'accurate' when looking at true news as they were when looking at false news. If the OR is `> 1`, then participants rated true news as more accurate than fake news. An OR of `2` means that participants were twice as likely to rate true news as accurate compared to false news.

The OR for skepticism  bias is calculated as

$$
OR_{Error} = \frac{(NotAccurate_{true}/Accurate_{true})}{(Accurate_{false}/NotAccurate_{false})} \leavevmode \newline
= \frac{\frac{1}{(NotAccurate_{true}/Accurate_{true})}}{(Accurate_{false}/NotAccurate_{false})} \leavevmode \newline
= \frac{1}{OR_{Accuracy}}
$$
For our analysis, we calculated the odds ratio (OR) for both accuracy and error. More precisely, we expressed the OR on a logarithmic scale, also referred to as "log odds ratio"(logOR). As for odds ratios, if the log odds ratio is positive, it indicates positive discernment/skepticism  bias^[To interpret the magnitude of that difference we have to transform the logarithmic estimate back to a normal odds ratio. The reason we use the log odds ratios in the first place is that which makes outcome measures symmetric around 0 and results in corresponding sampling distributions that are closer to normality [@viechtbauer_conducting_2010] ].

Table \@ref(tab:frequency) shows the frequency of answers by veracity.

```{r}
# We will make two version of individual level data. The main one only considers studies that actually used a binary response scale (`scale == binary`). The second takes all individual level data and computes binary responses for those studies that used a continuous scale. For example, on a 4-point scale, we would code 1 and 2 as not accurate (0) and 3 and 4 as accurate (1). For scales, with a mid-point (example 3 on a 5-point scale), we will code midpoint answers as NA.

# load individual level data
individual_level_subset <- read_csv("data/individual_level_subset.csv") %>% 
  filter(condition == "control") %>% 
  # transform accuracy scores to binary
  mutate(
    # add helper variable that indicates scales with midpoint
    midpoint_scale = ifelse(scale_numeric %% 2 != 0, TRUE, FALSE),
    accuracy = case_when(
      # keep binary scores
      scale == "binary"~accuracy, 
      # code midpoints as NA,
      midpoint_scale == TRUE & 
        accuracy == (scale_numeric/2)+0.5 ~ NA,
      # transform continuous scores
      accuracy <= scale_numeric/2 ~ 0, 
      accuracy > scale_numeric/2 ~ 1, 
      TRUE ~ NA)
  )

# make descriptive summary data for binary studies only 
descriptive_binary <- individual_level_subset %>%  
  filter(scale == "binary") %>% 
  group_by(veracity) %>% 
  summarize(
    sum_accuracy = sum(accuracy, na.rm = TRUE), 
    sum_NO_accuracy = sum(1 - accuracy, na.rm = TRUE)
  ) %>%
  rowwise() %>%
  mutate(
    total_sum = sum(sum_accuracy, sum_NO_accuracy),
    rel_accuracy = sum_accuracy / sum(sum_accuracy, sum_NO_accuracy),
    rel_NO_accuracy = sum_NO_accuracy / sum(sum_accuracy, sum_NO_accuracy), 
    total_rel = sum(rel_accuracy + rel_NO_accuracy)
  ) %>%
  ungroup() %>% 
  mutate_if(is.numeric, round, digits = 3) %>% 
  # add brackets around relative values
  mutate(across(c(rel_accuracy, rel_NO_accuracy, total_rel), ~paste0("(", ., ")"))) %>% 
      # unite absolute and relative values in the same variable
  mutate(accurate = paste0(sum_accuracy, " ", rel_accuracy), 
         not_accurate = paste0(sum_NO_accuracy, " ", rel_NO_accuracy),
         total = paste0(total_sum, " ", total_rel)
         ) %>% 
  select(veracity, accurate, not_accurate, total)
```

```{r frequency}
# define column names for a nicer table
new_names <- c("Veracity", "Rated as accurate", "Rated as not accurate", "Sum")
colnames(descriptive_binary) <- new_names

# summary table
apa_table(descriptive_binary , 
      note = "Frequency of responses (among individual-level studies with binary response scales)") 

```

```{r, echo=FALSE}
calculate_OR <- function(data, outcome) {
  
  # make a data frame that `escalc` can use to calculate logOR
  odds_ratios <- data %>%  
    group_by(unique_sample_id, veracity) %>% 
    summarize(
      sum_accuracy = sum(accuracy), 
      sum_NO_accuracy = sum(1-accuracy)
    ) %>% 
    pivot_wider(names_from = veracity, values_from = c(sum_accuracy, sum_NO_accuracy))
  
  if (outcome == "accuracy") {
  # calculate logOR using `escalc` for accuracy
  odds_ratios_accuracy <- escalc(measure="OR", 
                                 # true / fake
                                 ai= sum_accuracy_true, 
                                 bi=sum_NO_accuracy_true, 
                                 ci=sum_accuracy_fake,
                                 di=sum_NO_accuracy_fake,
                                 data = odds_ratios ) %>% 
    # add observation_id variable
    mutate(observation_id = 1:nrow(.)) 
  
  return(odds_ratios_accuracy)
  }
  
  if (outcome == "error") {
  # calculate logOR using `escalc` for error
  odds_ratios_error <- escalc(measure="OR", 
                              # true / fake
                              bi= sum_accuracy_true, 
                              ai=sum_NO_accuracy_true, 
                              ci=sum_accuracy_fake,
                              di=sum_NO_accuracy_fake,
                              data = odds_ratios ) %>% 
    # add observation_id variable
    mutate(observation_id = 1:nrow(.))
  
  return(odds_ratios_error)
  }
}
```

```{r}
# odds ratios for studies with binary response scales only
odds_ratios_accuracy <- calculate_OR(individual_level_subset %>% 
                                       filter(scale == "binary"), 
                                     outcome = "accuracy")
odds_ratios_error <- calculate_OR(individual_level_subset %>% 
                                    filter(scale == "binary"), 
                                  outcome = "error")

# odds ratios for all studies transposed to binary scale
odds_ratios_accuracy_all <- calculate_OR(individual_level_subset, 
                                     outcome = "accuracy")
odds_ratios_error_all <- calculate_OR(individual_level_subset, 
                                  outcome = "error")
```

### Meta-analysis 

We ran a meta-regression on the odds ratios. The results can be found in Table \@ref(tab:meta-odds). For reference, we also present the results for that subset using the estimator from the main analysis (Cohen's d) and a non-standardized estimator that likewise accounts for dependence between false and true news, namely the mean change (MC)^[We use the term mean change in line with vocabulary used by the metafor package and its `escalc()` function that we use for all effect size calculations. It is in fact a simple mean difference but one that accounts for the correlation between true and false news in the calculation of the standard error (see @higgins_cochrane_2019). Here is a direct link to the relevant chapter online: https://training.cochrane.org/handbook/current/chapter-23#section-23-2-7-1]. The results suggest that when using odds ratios, we do not find a statistically significant skepticism  bias. However, this analysis relies on very few observations (6 effect sizes only), hence low statistical power to detect a potential effect. We therefore extended the analysis by adding also individual-level studies with continuous response scales and collapsing ratings to a binary outcome. For example, on a 4-point scale, we coded responses of 1 and 2 as not accurate (0) and 3 and 4 as accurate (1). For scales, with a mid-point (example 3 on a 5-point scale), we coded midpoint answers as NA. The results of this extended analysis can be found in table \@ref(tab:meta-odds-extended). In line with our main results, this extended analysis suggests a positive skepticism  bias.

```{r}
compare_OR <- function(subset = "binary"){
  
  ##### SMCC (original meta analysis)
  
  # identify subset of individual_level_subset
  
  if(subset == "binary") {
    binary_individual_level_subset <- individual_level_subset %>% 
      filter(scale == "binary") %>% 
      reframe(unique(unique_sample_id)) %>% pull() 
  } else {
    binary_individual_level_subset <- individual_level_subset %>% 
      reframe(unique(unique_sample_id)) %>% pull() 
  }
  
  # restrict effect data frame to those binary studies
  reduce_binary_samples <- function(data) {
    
    results <- data %>% 
      filter(unique_sample_id %in% all_of(binary_individual_level_subset))
  }
  # get subset of relevant effect sizes
  binary_individual_accuracy_effect <- reduce_binary_samples(accuracy_effect)
  binary_individual_error_effect <- reduce_binary_samples(error_effect)
  
  # run model
  model_Cohensd_accuracy <- calculate_models(data=binary_individual_accuracy_effect, 
                                          robust = FALSE)
  model_Cohensd_error <- calculate_models(data=binary_individual_error_effect, 
                                       robust = FALSE)
  ##### Mean Difference (MD)
  
  # calculate (non-standardized) mean differences/change
  effect_MD_accuracy <- calculate_effect_sizes(effect = "accuracy", 
                                               measure = "MC",
                                               measure_between = "MD",
                                               data = meta_wide)
  effect_MD_error <- calculate_effect_sizes(effect = "error", 
                                            measure = "MC",
                                            measure_between = "MD",
                                            data = meta_wide)
  
  # get subset of relevant effect sizes
  binary_individual_accuracy_effect <- reduce_binary_samples(effect_MD_accuracy)
  binary_individual_error_effect <- reduce_binary_samples(effect_MD_error)
  
  
  # run model
  model_MD_accuracy <- calculate_models(data=binary_individual_accuracy_effect, 
                                        robust = FALSE)
  model_MD_error <- calculate_models(data=binary_individual_error_effect, 
                                     robust = FALSE)
  
  ##### OR
  
  if(subset == "binary") {
    
    # calculate models
    model_OR_accuracy <- calculate_models(data=odds_ratios_accuracy, robust = TRUE)
    model_OR_error <- calculate_models(data=odds_ratios_error, robust = TRUE)
  } else {
    # calculate models
    model_OR_accuracy <- calculate_models(data=odds_ratios_accuracy_all, robust = TRUE)
    model_OR_error <- calculate_models(data=odds_ratios_error_all, robust = TRUE)
  }
  
  return(list("Accuracy" = model_OR_accuracy, 
              "Error" = model_OR_error, 
              "Accuracy" = model_Cohensd_accuracy , 
              "Error" = model_Cohensd_error, 
              "Accuracy" = model_MD_accuracy , 
              "Error" = model_MD_error)
  )
}
```

```{r meta-odds}
comparison_binary <- compare_OR(subset = "binary") # change to subset = "all" for all individual-level studies collapsed to binary scale

# extract OR models
model_OR_accuracy <- comparison_binary[[1]]
model_OR_error <- comparison_binary[[2]]

modelsummary::modelsummary(comparison_binary,
                           title = 'Individual-level studies with binary response scale', 
                           stars = TRUE, 
                           coef_rename = c("overall" = "Estimate")
) %>% 
  add_header_above(c(" " = 1,
                     "Log OR" = 2,
                     "Cohen's d" = 2,
                     "Mean change" = 2),
                   ) %>%
  add_header_above(c(" " = 1, "(based on individual data)" = 2, "(based on meta data)"= 4),
                   line = FALSE, italic = TRUE) %>%
  footnote(general = "Note that the number of observations differ, because some samples provide several effect sizes in the meta-data. For the odds ratios based on the individual data, however, we calculated only one average effect size per sample. The samples are only from studies with binary response scales that we had raw, individual-level data on.",
           threeparttable = TRUE) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```

```{r meta-odds-extended}
comparison_binary <- compare_OR(subset = "all") 

# extract OR models
model_OR_accuracy <- comparison_binary[[1]]
model_OR_error <- comparison_binary[[2]]

modelsummary::modelsummary(comparison_binary,
                           title = 'Individual-level studies with likert scale ratings collapsed to binary outcome', 
                           stars = TRUE, 
                           coef_rename = c("overall" = "Estimate"),
                           escape = TRUE
) %>% 
  add_header_above(c(" " = 1,
                     "Log OR" = 2,
                     "Cohen's d" = 2,
                     "Mean change" = 2),
                   ) %>%
  add_header_above(c(" " = 1, "(based on individual data)" = 2, "(based on meta data)"= 4),
                   line = FALSE, italic = TRUE) %>%
  footnote(general = "Note that the number of observations differ, because some samples provide several effect sizes in the meta-data. For the odds ratios based on the individual data, however, we calculated only one average effect size per sample. The sample consists of all studies we had individual-level data on. For individual-level studies with continuous response scales, we computed the odds ratio after collapsing responses to a binary outcome.", threeparttable = TRUE) %>%
  # make smaller to fit
  kable_styling(latex_options = "scale_down")
```


